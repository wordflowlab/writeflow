/**
 * WriteFlow ç»Ÿä¸€æµå¼æœåŠ¡
 * é›†æˆæµå¼é€‚é…å™¨ä¸æ¨¡å‹ç®¡ç†çš„ç»Ÿä¸€æ¥å£
 */

import EventEmitter from 'events'
import { getModelCapabilities } from '../models/modelCapabilities.js'
import { getModelManager } from '../models/ModelManager.js'
import { logError } from '../../utils/log.js'
import { createStreamAdapterFromModel, StreamChunk, StreamAdapter } from './index.js'

export interface StreamingConfig {
  maxRetries?: number
  retryDelay?: number
  timeout?: number
  bufferSize?: number
  enableReconnect?: boolean
}

export interface StreamingRequest {
  prompt: string
  systemPrompt?: string
  model?: string
  maxTokens?: number
  temperature?: number
  allowedTools?: string[]
  enableToolCalls?: boolean
}

export interface StreamingResponse {
  content: string
  reasoning?: string
  usage?: {
    inputTokens: number
    outputTokens: number
    totalTokens?: number
  }
  cost?: number
  duration?: number
  model: string
  done: boolean
  error?: string
  toolCalls?: any[]
}

/**
 * UI ä¼˜åŒ–çš„æµå¼å—äº‹ä»¶
 */
export interface UIStreamingChunk {
  streamId: string
  content: string
  delta: string // æ–°å¢å†…å®¹
  timestamp: number
  characterCount: number
  renderHint: {
    contentType: 'text' | 'markdown' | 'code' | 'mixed'
    suggestedDelay: number // å»ºè®®çš„æ¸²æŸ“å»¶è¿Ÿ
    priority: 'low' | 'normal' | 'high' | 'urgent'
  }
  performance: {
    networkLatency: number
    processingTime: number
    bufferSize: number
  }
}

/**
 * æ¸²æŸ“æ›´æ–°äº‹ä»¶
 */
export interface RenderUpdateEvent {
  streamId: string
  renderStats: {
    fps: number
    averageFrameTime: number
    droppedFrames: number
    totalFramesRendered: number
  }
  bufferStats: {
    size: number
    utilizationPercent: number
    isOverflowing: boolean
  }
  recommendations?: string[]
}

/**
 * æ€§èƒ½è­¦å‘Šäº‹ä»¶
 */
export interface PerformanceWarningEvent {
  streamId: string
  type: 'fps_drop' | 'high_latency' | 'buffer_overflow' | 'memory_pressure' | 'render_lag'
  severity: 'info' | 'warning' | 'error' | 'critical'
  message: string
  metrics: {
    currentValue: number
    threshold: number
    trend: 'improving' | 'stable' | 'degrading'
  }
  timestamp: number
  autoActions?: string[] // ç³»ç»Ÿè‡ªåŠ¨é‡‡å–çš„æªæ–½
}

/**
 * å†…å®¹è§£æè¿›åº¦äº‹ä»¶
 */
export interface ParsingProgressEvent {
  streamId: string
  totalCharacters: number
  parsedCharacters: number
  progress: number // 0-100
  detectedElements: {
    headings: number
    codeBlocks: number
    lists: number
    links: number
    tables: number
  }
  parsingQuality: 'excellent' | 'good' | 'fair' | 'poor'
  estimatedTimeRemaining?: number
}

/**
 * UI æ€§èƒ½é˜ˆå€¼é…ç½®
 */
export interface UIPerformanceThresholds {
  maxLatency: number // æœ€å¤§å¯æ¥å—å»¶è¿Ÿ (ms)
  minFPS: number // æœ€å°å¯æ¥å—å¸§ç‡
  maxBufferSize: number // æœ€å¤§ç¼“å†²åŒºå¤§å° (bytes)
  warningLatency: number // å»¶è¿Ÿè­¦å‘Šé˜ˆå€¼ (ms)
}

/**
 * æ€§èƒ½ç›‘æ§å™¨
 */
export interface RenderStats {
  fps: number
  averageFrameTime: number
  droppedFrames: number
  timestamp: number
}

export class PerformanceMonitor {
  private frameTimeHistory: number[] = []
  private lastFrameTime = 0
  private droppedFrames = 0
  private renderCallbacks: ((stats: RenderStats) => void)[] = []
  
  recordFrame(timestamp: number): void {
    if (this.lastFrameTime > 0) {
      const frameTime = timestamp - this.lastFrameTime
      this.frameTimeHistory.push(frameTime)
      
      // ä¿æŒæœ€è¿‘ 60 å¸§çš„å†å²
      if (this.frameTimeHistory.length > 60) {
        this.frameTimeHistory.shift()
      }
      
      // æ£€æµ‹æ‰å¸§ï¼ˆå‡è®¾ç›®æ ‡æ˜¯60fpsï¼Œ16.67ms per frameï¼‰
      if (frameTime > 33) { // è¶…è¿‡2å¸§æ—¶é—´è®¤ä¸ºæ‰å¸§
        this.droppedFrames++
      }
    }
    this.lastFrameTime = timestamp
    
    // é€šçŸ¥ç›‘å¬å™¨
    const stats = this.getCurrentStats(timestamp)
    this.renderCallbacks.forEach(callback => callback(stats))
  }
  
  getCurrentStats(timestamp: number): RenderStats {
    const avgFrameTime = this.frameTimeHistory.length > 0 
      ? this.frameTimeHistory.reduce((a, b) => a + b) / this.frameTimeHistory.length
      : 0
    
    return {
      fps: avgFrameTime > 0 ? 1000 / avgFrameTime : 0,
      averageFrameTime: avgFrameTime,
      droppedFrames: this.droppedFrames,
      timestamp
    }
  }
  
  onRender(callback: (stats: RenderStats) => void): void {
    this.renderCallbacks.push(callback)
  }
  
  reset(): void {
    this.frameTimeHistory = []
    this.lastFrameTime = 0
    this.droppedFrames = 0
  }
}

/**
 * æµå¼æœåŠ¡äº‹ä»¶
 */
export interface StreamingServiceEvents {
  chunk: [StreamingResponse]
  complete: [StreamingResponse]
  error: [Error]
  
  // UI ä¸“ç”¨äº‹ä»¶
  uiChunk: [UIStreamingChunk] // ä¸“é—¨ä¸º UI ä¼˜åŒ–çš„å—äº‹ä»¶
  renderUpdate: [RenderUpdateEvent] // æ¸²æŸ“æ›´æ–°äº‹ä»¶
  performanceWarning: [PerformanceWarningEvent] // æ€§èƒ½è­¦å‘Šäº‹ä»¶
  parsingProgress: [ParsingProgressEvent] // å†…å®¹è§£æè¿›åº¦äº‹ä»¶
}

/**
 * WriteFlow æµå¼æœåŠ¡
 */
export class StreamingService extends EventEmitter {
  private modelManager = getModelManager()
  private currentAdapter?: StreamAdapter
  private config: StreamingConfig
  private retryCount = 0
  private isStreaming = false
  
  // UI å¢å¼ºç›¸å…³å­—æ®µ
  private currentStreamId?: string
  private streamStartTime = 0
  private lastChunkTime = 0
  private accumulatedContent = ''
  private performanceMonitor: PerformanceMonitor
  private uiThresholds: UIPerformanceThresholds
  
  constructor(config: StreamingConfig = {}) {
    super()
    this.config = {
      maxRetries: 3,
      retryDelay: 1000,
      timeout: 60000,
      bufferSize: 8192,
      enableReconnect: true,
      ...config
    }
    
    // åˆå§‹åŒ– UI å¢å¼ºåŠŸèƒ½
    this.performanceMonitor = new PerformanceMonitor()
    this.uiThresholds = {
      maxLatency: 2000,
      minFPS: 15,
      maxBufferSize: 64 * 1024, // 64KB
      warningLatency: 1000
    }
    
    // è®¾ç½®æ€§èƒ½ç›‘æ§å›è°ƒ
    this.performanceMonitor.onRender((stats) => {
      this.handlePerformanceUpdate(stats)
    })
  }
  
  /**
   * å¼€å§‹æµå¼è¯·æ±‚
   */
  async startStream(request: StreamingRequest): Promise<void> {
    if (this.isStreaming) {
      throw new Error('å·²æœ‰æµå¼è¯·æ±‚æ­£åœ¨è¿›è¡Œä¸­ï¼Œè¯·å…ˆåœæ­¢å½“å‰è¯·æ±‚')
    }
    
    this.isStreaming = true
    this.retryCount = 0
    
    // åˆå§‹åŒ–æµå¼ä¼šè¯
    this.initializeStreamingSession()
    
    await this.attemptStream(request)
  }

  /**
   * åˆå§‹åŒ–æµå¼ä¼šè¯
   */
  private initializeStreamingSession(): void {
    this.currentStreamId = `stream_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`
    this.streamStartTime = Date.now()
    this.lastChunkTime = 0
    this.accumulatedContent = ''
    this.performanceMonitor.reset()
  }

  /**
   * å¤„ç†æ€§èƒ½æ›´æ–°
   */
  private handlePerformanceUpdate(stats: RenderStats): void {
    if (!this.currentStreamId) return
    
    // å‘å‡ºæ¸²æŸ“æ›´æ–°äº‹ä»¶
    const renderUpdate: RenderUpdateEvent = {
      streamId: this.currentStreamId,
      renderStats: {
        fps: stats.fps,
        averageFrameTime: stats.averageFrameTime,
        droppedFrames: stats.droppedFrames,
        totalFramesRendered: 0 // éœ€è¦åœ¨å…·ä½“å®ç°ä¸­è®¡ç®—
      },
      bufferStats: {
        size: this.accumulatedContent.length,
        utilizationPercent: Math.min(100, (this.accumulatedContent.length / this.uiThresholds.maxBufferSize) * 100),
        isOverflowing: this.accumulatedContent.length > this.uiThresholds.maxBufferSize
      }
    }
    
    this.emit('renderUpdate', renderUpdate)
    
    // æ£€æŸ¥æ€§èƒ½è­¦å‘Š
    this.checkPerformanceWarnings(stats)
  }

  /**
   * æ£€æŸ¥æ€§èƒ½è­¦å‘Š
   */
  private checkPerformanceWarnings(stats: RenderStats): void {
    if (!this.currentStreamId) return
    
    const now = Date.now()
    
    // FPS è¿‡ä½è­¦å‘Š
    if (stats.fps < this.uiThresholds.minFPS) {
      const warning: PerformanceWarningEvent = {
        streamId: this.currentStreamId,
        type: 'fps_drop',
        severity: stats.fps < 10 ? 'critical' : 'warning',
        message: `å¸§ç‡è¿‡ä½: ${stats.fps.toFixed(1)}fps (ç›®æ ‡: ${this.uiThresholds.minFPS}fps)`,
        metrics: {
          currentValue: stats.fps,
          threshold: this.uiThresholds.minFPS,
          trend: this.analyzeTrend(stats.fps, 'fps')
        },
        timestamp: now,
        autoActions: ['é™ä½æ¸²æŸ“è´¨é‡', 'å¢åŠ æ¸²æŸ“é—´éš”']
      }
      this.emit('performanceWarning', warning)
    }
    
    // ç¼“å†²åŒºæº¢å‡ºè­¦å‘Š
    if (this.accumulatedContent.length > this.uiThresholds.maxBufferSize) {
      const warning: PerformanceWarningEvent = {
        streamId: this.currentStreamId,
        type: 'buffer_overflow',
        severity: 'error',
        message: `ç¼“å†²åŒºæº¢å‡º: ${this.accumulatedContent.length} bytes (æœ€å¤§: ${this.uiThresholds.maxBufferSize})`,
        metrics: {
          currentValue: this.accumulatedContent.length,
          threshold: this.uiThresholds.maxBufferSize,
          trend: 'degrading'
        },
        timestamp: now,
        autoActions: ['æ¸…ç†æ—§æ•°æ®', 'å¢åŠ ç¼“å†²åŒºå¤§å°']
      }
      this.emit('performanceWarning', warning)
    }
  }

  /**
   * åˆ†æè¶‹åŠ¿
   */
  private analyzeTrend(currentValue: number, metricType: string): 'improving' | 'stable' | 'degrading' {
    // ç®€åŒ–çš„è¶‹åŠ¿åˆ†æï¼Œå®é™…ä¸­å¯ä»¥ç»´æŠ¤å†å²æ•°æ®è¿›è¡Œæ›´å¤æ‚çš„åˆ†æ
    return 'stable' // é»˜è®¤è¿”å›ç¨³å®š
  }
  
  /**
   * å°è¯•æµå¼è¯·æ±‚ï¼ˆæ”¯æŒé‡è¯•ï¼‰
   */
  private async attemptStream(request: StreamingRequest): Promise<void> {
    const startTime = Date.now()
    
    try {
      // è·å–æ¨¡å‹é…ç½®
      const modelName = request.model || this.modelManager.getMainAgentModel()
      if (!modelName) {
        throw new Error('æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹é…ç½®')
      }
      
      const capabilities = getModelCapabilities(modelName)
      
      // æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒæµå¼
      if (!capabilities.supportsStreaming || !capabilities.streamingProtocol) {
        throw new Error(`æ¨¡å‹ ${modelName} ä¸æ”¯æŒæµå¼å“åº”`)
      }
      
      // åˆ›å»ºæµå¼é€‚é…å™¨
      this.currentAdapter = createStreamAdapterFromModel(modelName, {
        bufferSize: this.config.bufferSize,
        parseStrategy: 'incremental',
        timeout: this.config.timeout
      })
      
      // è®¾ç½®é€‚é…å™¨çš„æµ IDï¼Œç¡®ä¿äº‹ä»¶å…³è”æ­£ç¡®
      if (this.currentAdapter && this.currentStreamId) {
        this.currentAdapter.setStreamId(this.currentStreamId)
      }
      
      // è®¾ç½®é€‚é…å™¨äº‹ä»¶ç›‘å¬
      this.setupAdapterListeners(startTime, modelName, request)
      
      // æ ¹æ®æ¨¡å‹èƒ½åŠ›æ„å»ºè¯·æ±‚
      const apiRequest = await this.buildAPIRequest(request, capabilities)
      
      // å‘èµ· API è¯·æ±‚
      await this.makeStreamingRequest(apiRequest)
      
    } catch (error) {
      await this.handleStreamError(error, request)
    }
  }
  
  /**
   * å¤„ç†æµå¼é”™è¯¯
   */
  private async handleStreamError(error: any, request: StreamingRequest): Promise<void> {
    logError('æµå¼è¯·æ±‚é”™è¯¯', error)
    
    // æ£€æŸ¥æ˜¯å¦å¯ä»¥é‡è¯•
    if (this.config.enableReconnect && 
        this.retryCount < (this.config.maxRetries || 3) && 
        this.isRetryableError(error)) {
      
      this.retryCount++
      const delay = (this.config.retryDelay || 1000) * Math.pow(2, this.retryCount - 1) // æŒ‡æ•°é€€é¿
      
      console.log(`ğŸ”„ æµå¼è¯·æ±‚å¤±è´¥ï¼Œ${delay}ms åè¿›è¡Œç¬¬ ${this.retryCount} æ¬¡é‡è¯•`)
      
      // æ¸…ç†å½“å‰é€‚é…å™¨
      this.cleanup()
      
      // å»¶æ—¶é‡è¯•
      setTimeout(() => {
        if (this.isStreaming) {
          this.attemptStream(request).catch(() => {
            this.emitFinalError(error)
          })
        }
      }, delay)
      
    } else {
      this.emitFinalError(error)
    }
  }
  
  /**
   * åˆ¤æ–­æ˜¯å¦ä¸ºå¯é‡è¯•çš„é”™è¯¯
   */
  private isRetryableError(error: any): boolean {
    const errorMessage = error?.message?.toLowerCase() || ''
    const retryableErrors = [
      'network',
      'timeout',
      'connection',
      'socket',
      'econnreset',
      'enotfound',
      'econnrefused',
      '502',
      '503',
      '504'
    ]
    
    return retryableErrors.some(pattern => errorMessage.includes(pattern))
  }
  
  /**
   * å‘å‡ºæœ€ç»ˆé”™è¯¯
   */
  private emitFinalError(error: any): void {
    this.isStreaming = false
    this.cleanup()
    this.emit('error', error instanceof Error ? error : new Error(String(error)))
  }
  
  /**
   * åœæ­¢å½“å‰æµå¼è¯·æ±‚
   */
  stopStream(): void {
    this.isStreaming = false
    this.cleanup()
  }
  
  /**
   * è®¾ç½®é€‚é…å™¨äº‹ä»¶ç›‘å¬
   */
  private setupAdapterListeners(startTime: number, modelName: string, request: StreamingRequest): void {
    if (!this.currentAdapter) return
    
    let accumulatedContent = ''
    let accumulatedReasoning = ''
    let finalUsage: any = null
    
    this.currentAdapter.on('chunk', (chunk: StreamChunk) => {
      const chunkTime = Date.now()
      const previousLength = accumulatedContent.length
      
      accumulatedContent += chunk.content
      accumulatedReasoning += chunk.reasoning || ''
      
      // æ›´æ–°å†…éƒ¨çŠ¶æ€
      this.accumulatedContent = accumulatedContent
      
      if (chunk.usage) {
        finalUsage = chunk.usage
      }
      
      // è®¡ç®—ç½‘ç»œå»¶è¿Ÿ
      const networkLatency = this.lastChunkTime > 0 ? chunkTime - this.lastChunkTime : 0
      this.lastChunkTime = chunkTime
      
      // è®°å½•æ¸²æŸ“å¸§ï¼ˆç”¨äºæ€§èƒ½ç›‘æ§ï¼‰
      this.performanceMonitor.recordFrame(chunkTime)
      
      if (chunk.error) {
        const response: StreamingResponse = {
          content: accumulatedContent,
          reasoning: accumulatedReasoning || undefined,
          usage: finalUsage || undefined,
          model: modelName,
          done: chunk.done,
          error: chunk.error
        }
        this.handleStreamError(new Error(chunk.error), request)
        return
      }
      
      // å­—ç¬¦çº§å¢é‡å‘å°„ - å…³é”®æ”¹è¿›ï¼šä¸ºæ¯ä¸ªæ–°å­—ç¬¦å‘å‡º UI äº‹ä»¶
      const newContent = chunk.content
      if (newContent && newContent.length > 0) {
        // ä¸ºæ–°å¢å†…å®¹çš„æ¯ä¸ªå­—ç¬¦å‘å‡ºç‹¬ç«‹çš„ UI äº‹ä»¶
        this.emitCharacterLevelEvents(
          newContent, 
          accumulatedContent,
          previousLength,
          chunkTime,
          startTime,
          networkLatency
        )
      }
      
      // åˆ›å»ºæ ‡å‡†å“åº”ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
      const response: StreamingResponse = {
        content: accumulatedContent,
        reasoning: accumulatedReasoning || undefined,
        usage: finalUsage || undefined,
        model: modelName,
        done: chunk.done
      }
      
      // å‘å‡ºæ ‡å‡†å—äº‹ä»¶ï¼ˆè¾ƒä½é¢‘ç‡ï¼‰
      this.emit('chunk', response)
      
      // æ£€æŸ¥æ˜¯å¦éœ€è¦å‘å‡ºè§£æè¿›åº¦äº‹ä»¶
      if (accumulatedContent.length > previousLength + 50) { // é™ä½åˆ°50å­—ç¬¦ï¼Œæé«˜é¢‘ç‡
        this.emitParsingProgress(accumulatedContent)
      }
      
      if (chunk.done) {
        response.duration = Date.now() - startTime
        response.cost = this.calculateCost(finalUsage, modelName)
        this.isStreaming = false
        
        // æœ€ç»ˆè§£æè¿›åº¦
        this.emitParsingProgress(accumulatedContent, true)
        
        this.emit('complete', response)
        this.cleanup()
      }
    })
    
    this.currentAdapter.on('error', (error: Error) => {
      this.handleStreamError(error, request)
    })
  }

  /**
   * æ£€æµ‹å†…å®¹ç±»å‹
   */
  private detectContentType(content: string): 'text' | 'markdown' | 'code' | 'mixed' {
    if (content.includes('```')) return 'code'
    if (content.includes('#') || content.includes('*') || content.includes('- ')) return 'markdown'
    return 'text'
  }

  /**
   * è®¡ç®—å»ºè®®çš„æ¸²æŸ“å»¶è¿Ÿ
   */
  private calculateSuggestedDelay(networkLatency: number): number {
    // åŸºäºç½‘ç»œå»¶è¿Ÿè°ƒæ•´æ¸²æŸ“é€Ÿåº¦
    if (networkLatency < 100) return 10
    if (networkLatency < 500) return 15
    if (networkLatency < 1000) return 25
    return 50
  }

  /**
   * ç¡®å®šä¼˜å…ˆçº§
   */
  private determinePriority(content: string, totalLength: number): 'low' | 'normal' | 'high' | 'urgent' {
    if (content.includes('error') || content.includes('ERROR')) return 'urgent'
    if (content.includes('warning') || content.includes('WARNING')) return 'high'
    if (totalLength < 100) return 'high' // å¼€å§‹æ—¶ä¼˜å…ˆçº§é«˜
    return 'normal'
  }

  /**
   * å‘å‡ºè§£æè¿›åº¦äº‹ä»¶
   */
  private emitParsingProgress(content: string, isComplete = false): void {
    if (!this.currentStreamId) return
    
    // ç®€å•çš„ç»“æ„åŒ–å…ƒç´ è®¡æ•°
    const headings = (content.match(/#{1,6}\s/g) || []).length
    const codeBlocks = (content.match(/```/g) || []).length / 2
    const lists = (content.match(/^[\s]*[-*+]\s/gm) || []).length
    const links = (content.match(/\[.*\]\(.*\)/g) || []).length
    const tables = (content.match(/\|.*\|/g) || []).length
    
    const progress = isComplete ? 100 : Math.min(95, (content.length / 2000) * 100) // å‡è®¾2000å­—ç¬¦ä¸ºå®Œæ•´å†…å®¹
    
    const parsingEvent: ParsingProgressEvent = {
      streamId: this.currentStreamId,
      totalCharacters: content.length,
      parsedCharacters: content.length,
      progress,
      detectedElements: {
        headings,
        codeBlocks: Math.floor(codeBlocks),
        lists,
        links,
        tables
      },
      parsingQuality: this.assessParsingQuality(headings, codeBlocks, lists),
      estimatedTimeRemaining: isComplete ? 0 : this.estimateRemainingTime(progress)
    }
    
    this.emit('parsingProgress', parsingEvent)
  }

  /**
   * è¯„ä¼°è§£æè´¨é‡
   */
  private assessParsingQuality(headings: number, codeBlocks: number, lists: number): 'excellent' | 'good' | 'fair' | 'poor' {
    const totalElements = headings + codeBlocks + lists
    if (totalElements >= 10) return 'excellent'
    if (totalElements >= 5) return 'good'
    if (totalElements >= 2) return 'fair'
    return 'poor'
  }

  /**
   * ä¼°ç®—å‰©ä½™æ—¶é—´
   */
  private estimateRemainingTime(progress: number): number | undefined {
    if (progress <= 0) return undefined
    
    const elapsed = Date.now() - this.streamStartTime
    const estimatedTotal = elapsed / (progress / 100)
    return Math.max(0, estimatedTotal - elapsed)
  }

  /**
   * å‘å‡ºå­—ç¬¦çº§ UI äº‹ä»¶ - å®ç°çœŸæ­£çš„æµå¼æ¸²æŸ“
   */
  private emitCharacterLevelEvents(
    newContent: string,
    fullContent: string,
    previousLength: number,
    chunkTime: number,
    startTime: number,
    networkLatency: number
  ): void {
    if (!this.currentStreamId || newContent.length === 0) return

    // å­—ç¬¦çº§å‘å°„ç­–ç•¥ï¼š
    // 1. å¯¹äºçŸ­å†…å®¹ï¼ˆ< 10å­—ç¬¦ï¼‰ï¼Œæ¯ä¸ªå­—ç¬¦å•ç‹¬å‘å‡º
    // 2. å¯¹äºä¸­ç­‰å†…å®¹ï¼ˆ10-50å­—ç¬¦ï¼‰ï¼ŒæŒ‰2-3ä¸ªå­—ç¬¦ä¸ºç»„å‘å‡º  
    // 3. å¯¹äºé•¿å†…å®¹ï¼ˆ> 50å­—ç¬¦ï¼‰ï¼ŒæŒ‰5-8ä¸ªå­—ç¬¦ä¸ºç»„å‘å‡º
    
    let chunkSize = 1 // é»˜è®¤å•å­—ç¬¦
    if (newContent.length > 50) {
      chunkSize = Math.min(8, Math.max(5, Math.ceil(newContent.length / 10)))
    } else if (newContent.length > 10) {
      chunkSize = Math.min(3, Math.max(2, Math.ceil(newContent.length / 5)))
    }

    // åˆ†æ‰¹å‘å‡ºå­—ç¬¦çº§äº‹ä»¶
    for (let i = 0; i < newContent.length; i += chunkSize) {
      const characterBatch = newContent.slice(i, i + chunkSize)
      const currentPosition = previousLength + i + characterBatch.length

      // æ¨¡æ‹Ÿæ¸è¿›å¼æ—¶é—´æˆ³ï¼ˆè®©UIæœ‰æ—¶é—´æ¸²æŸ“ï¼‰
      const syntheticTimestamp = chunkTime + (i / newContent.length) * 10 // åˆ†æ•£åœ¨10mså†…

      const uiChunk: UIStreamingChunk = {
        streamId: this.currentStreamId,
        content: fullContent.slice(0, currentPosition), // å½“å‰å®Œæ•´å†…å®¹
        delta: characterBatch, // æœ¬æ¬¡æ–°å¢çš„å­—ç¬¦
        timestamp: syntheticTimestamp,
        characterCount: currentPosition,
        renderHint: {
          contentType: this.detectContentType(characterBatch),
          suggestedDelay: this.calculateCharacterDelay(networkLatency, i, newContent.length),
          priority: this.determinePriority(characterBatch, currentPosition)
        },
        performance: {
          networkLatency,
          processingTime: syntheticTimestamp - startTime,
          bufferSize: currentPosition
        }
      }

      // ç«‹å³å‘å‡º UI äº‹ä»¶
      this.emit('uiChunk', uiChunk)
    }
  }

  /**
   * è®¡ç®—å­—ç¬¦çº§æ¸²æŸ“å»¶è¿Ÿ
   */
  private calculateCharacterDelay(networkLatency: number, characterIndex: number, totalCharacters: number): number {
    // åŸºç¡€å»¶è¿Ÿæ ¹æ®ç½‘ç»œçŠ¶å†µè°ƒæ•´
    let baseDelay = 15 // é»˜è®¤15ms

    if (networkLatency < 100) {
      baseDelay = 8  // ç½‘ç»œå¿«æ—¶æ›´å¿«æ¸²æŸ“
    } else if (networkLatency < 500) {
      baseDelay = 12
    } else if (networkLatency > 1000) {
      baseDelay = 25 // ç½‘ç»œæ…¢æ—¶ç¨å¾®æ…¢ä¸€ç‚¹
    }

    // åœ¨æ‰¹æ¬¡ä¸­é—´çš„å­—ç¬¦æ¸²æŸ“æ›´å¿«ï¼ˆæ¨¡æ‹Ÿæ‰“å­—æ•ˆæœï¼‰
    const positionFactor = characterIndex < totalCharacters / 2 ? 0.8 : 1.0
    
    return Math.round(baseDelay * positionFactor)
  }
  
  /**
   * æ„å»º API è¯·æ±‚å‚æ•°
   */
  private async buildAPIRequest(request: StreamingRequest, capabilities: any): Promise<any> {
    const modelProfile = this.findModelProfile(request.model)
    if (!modelProfile) {
      throw new Error(`æ‰¾ä¸åˆ°æ¨¡å‹é…ç½®: ${request.model}`)
    }
    
    const messages = []
    if (request.systemPrompt && capabilities.supportsSystemMessages) {
      if (capabilities.streamingProtocol.format === 'anthropic') {
        // Anthropic ä½¿ç”¨å•ç‹¬çš„ system å­—æ®µ
        return {
          model: modelProfile.modelName,
          max_tokens: request.maxTokens || modelProfile.maxTokens || 4096,
          temperature: request.temperature || 0.3,
          messages: [{ role: 'user', content: request.prompt }],
          system: request.systemPrompt,
          stream: true
        }
      } else {
        messages.push({ role: 'system', content: request.systemPrompt })
      }
    }
    
    messages.push({ role: 'user', content: request.prompt })
    
    const baseRequest = {
      model: modelProfile.modelName,
      messages,
      max_tokens: request.maxTokens || modelProfile.maxTokens || 4096,
      temperature: request.temperature || 0.3,
      stream: true
    }
    
    // æ·»åŠ å·¥å…·è°ƒç”¨æ”¯æŒ
    if (request.enableToolCalls && request.allowedTools && capabilities.supportsFunctionCalling) {
      // TODO: å®ç°å·¥å…·å®šä¹‰è½¬æ¢
      // baseRequest.tools = await this.convertToolsToAPIFormat(request.allowedTools, capabilities.streamingProtocol.format)
    }
    
    return baseRequest
  }
  
  /**
   * å‘èµ·æµå¼ API è¯·æ±‚
   */
  private async makeStreamingRequest(apiRequest: any): Promise<void> {
    const modelProfile = this.findModelProfile(apiRequest.model)
    if (!modelProfile) {
      throw new Error(`æ‰¾ä¸åˆ°æ¨¡å‹é…ç½®: ${apiRequest.model}`)
    }
    
    const apiKey = this.getAPIKey(modelProfile)
    if (!apiKey) {
      throw new Error(`ç¼ºå°‘ ${modelProfile.provider} API å¯†é’¥`)
    }
    
    const url = this.getAPIURL(modelProfile)
    const headers = this.getAPIHeaders(modelProfile, apiKey)
    
    try {
      const response = await fetch(url, {
        method: 'POST',
        headers,
        body: JSON.stringify(apiRequest)
      })
      
      if (!response.ok) {
        const errorText = await response.text()
        throw new Error(`API é”™è¯¯: ${response.status} - ${errorText}`)
      }
      
      if (!response.body) {
        throw new Error('å“åº”ä½“ä¸ºç©º')
      }
      
      // å¤„ç†æµå¼å“åº”
      await this.processStreamResponse(response.body)
      
    } catch (error) {
      logError('æµå¼è¯·æ±‚å¤±è´¥', error)
      throw error
    }
  }
  
  /**
   * å¤„ç†æµå¼å“åº”
   */
  private async processStreamResponse(body: ReadableStream<Uint8Array>): Promise<void> {
    const reader = body.getReader()
    const decoder = new TextDecoder()
    
    try {
      while (true) {
        const { done, value } = await reader.read()
        
        if (done) break
        
        const chunk = decoder.decode(value, { stream: true })
        
        // ä½¿ç”¨é€‚é…å™¨å¤„ç†æ•°æ®å—
        if (this.currentAdapter) {
          this.currentAdapter.processData(chunk)
        }
      }
    } finally {
      reader.releaseLock()
    }
  }
  
  /**
   * è·å– API URL
   */
  private getAPIURL(modelProfile: any): string {
    switch (modelProfile.provider) {
      case 'anthropic':
      case 'bigdream':
        return modelProfile.baseURL || 'https://api.anthropic.com/v1/messages'
      case 'deepseek':
        return modelProfile.baseURL || 'https://api.deepseek.com/chat/completions'
      case 'openai':
        return modelProfile.baseURL || 'https://api.openai.com/v1/chat/completions'
      case 'kimi':
        return modelProfile.baseURL || 'https://api.moonshot.cn/v1/chat/completions'
      default:
        throw new Error(`ä¸æ”¯æŒçš„æä¾›å•†: ${modelProfile.provider}`)
    }
  }
  
  /**
   * è·å– API è¯·æ±‚å¤´
   */
  private getAPIHeaders(modelProfile: any, apiKey: string): Record<string, string> {
    const commonHeaders = { 'Content-Type': 'application/json' }
    
    switch (modelProfile.provider) {
      case 'anthropic':
      case 'bigdream':
        return {
          ...commonHeaders,
          'x-api-key': apiKey,
          'anthropic-version': '2023-06-01'
        }
      case 'deepseek':
      case 'openai':
      case 'kimi':
        return {
          ...commonHeaders,
          'Authorization': `Bearer ${apiKey}`
        }
      default:
        return commonHeaders
    }
  }
  
  /**
   * è·å– API å¯†é’¥
   */
  private getAPIKey(modelProfile: any): string | undefined {
    if (modelProfile.apiKey) {
      return modelProfile.apiKey
    }
    
    const envKeys = {
      anthropic: ['ANTHROPIC_API_KEY', 'CLAUDE_API_KEY'],
      bigdream: ['ANTHROPIC_API_KEY', 'CLAUDE_API_KEY'],
      deepseek: ['DEEPSEEK_API_KEY'],
      openai: ['OPENAI_API_KEY'],
      kimi: ['KIMI_API_KEY', 'MOONSHOT_API_KEY']
    }
    
    const keys = envKeys[modelProfile.provider as keyof typeof envKeys] || []
    return keys.map(key => process.env[key]).find(Boolean)
  }
  
  /**
   * æŸ¥æ‰¾æ¨¡å‹é…ç½®
   */
  private findModelProfile(modelName?: string): any {
    if (!modelName) return null
    
    const profiles = this.modelManager.getAllProfiles()
    return profiles.find(p => p.modelName === modelName || p.name === modelName) || null
  }
  
  /**
   * è®¡ç®—æˆæœ¬
   */
  private calculateCost(usage: any, modelName: string): number {
    if (!usage) return 0
    
    const capabilities = getModelCapabilities(modelName)
    if (!capabilities.pricing) return 0
    
    const inputTokens = usage.promptTokens || usage.inputTokens || 0
    const outputTokens = usage.completionTokens || usage.outputTokens || 0
    
    return inputTokens * capabilities.pricing.inputCostPerToken + 
           outputTokens * capabilities.pricing.outputCostPerToken
  }
  
  /**
   * æ¸…ç†èµ„æº
   */
  private cleanup(): void {
    if (this.currentAdapter) {
      this.currentAdapter.removeAllListeners()
      this.currentAdapter.reset()
      this.currentAdapter = undefined
    }
  }
  
  /**
   * è·å–å½“å‰æµå¼çŠ¶æ€
   */
  getStreamingStatus(): {
    isStreaming: boolean
    retryCount: number
    config: StreamingConfig
  } {
    return {
      isStreaming: this.isStreaming,
      retryCount: this.retryCount,
      config: { ...this.config }
    }
  }
  
  /**
   * æ›´æ–°æµå¼é…ç½®
   */
  updateConfig(config: Partial<StreamingConfig>): void {
    this.config = { ...this.config, ...config }
  }

  /**
   * è·å–å½“å‰æµçš„ ID
   */
  getCurrentStreamId(): string | undefined {
    return this.currentStreamId
  }

  /**
   * è®¾ç½® UI æ€§èƒ½é˜ˆå€¼
   */
  setUIThresholds(thresholds: Partial<UIPerformanceThresholds>): void {
    this.uiThresholds = { ...this.uiThresholds, ...thresholds }
  }

  /**
   * è·å–å½“å‰æ€§èƒ½ç»Ÿè®¡
   */
  getCurrentPerformanceStats(): RenderStats | null {
    if (!this.isStreaming) return null
    return this.performanceMonitor.getCurrentStats(Date.now())
  }

  /**
   * è·å–æµå¼ä¼šè¯ä¿¡æ¯
   */
  getSessionInfo(): {
    streamId?: string
    isStreaming: boolean
    startTime: number
    duration: number
    accumulatedLength: number
  } {
    return {
      streamId: this.currentStreamId,
      isStreaming: this.isStreaming,
      startTime: this.streamStartTime,
      duration: this.streamStartTime > 0 ? Date.now() - this.streamStartTime : 0,
      accumulatedLength: this.accumulatedContent.length
    }
  }

  /**
   * æ‰‹åŠ¨è§¦å‘æ€§èƒ½æ£€æŸ¥
   */
  triggerPerformanceCheck(): void {
    if (this.isStreaming) {
      const stats = this.performanceMonitor.getCurrentStats(Date.now())
      this.checkPerformanceWarnings(stats)
    }
  }
}

// å…¨å±€æœåŠ¡å®ä¾‹
let globalStreamingService: StreamingService | null = null

/**
 * è·å–å…¨å±€æµå¼æœåŠ¡å®ä¾‹
 */
export function getStreamingService(config?: StreamingConfig): StreamingService {
  if (!globalStreamingService) {
    globalStreamingService = new StreamingService(config)
  } else if (config) {
    globalStreamingService.updateConfig(config)
  }
  return globalStreamingService
}

/**
 * å¿«é€Ÿæµå¼è¯·æ±‚å‡½æ•°
 */
export async function streamAI(
  prompt: string, 
  options?: Partial<StreamingRequest>
): Promise<StreamingService> {
  const service = getStreamingService()
  
  await service.startStream({
    prompt,
    ...options
  })
  
  return service
}