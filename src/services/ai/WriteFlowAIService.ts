/**
 * WriteFlow AI æœåŠ¡ - é‡æ„ç‰ˆæœ¬
 * ä¸“ä¸ºå†™ä½œåœºæ™¯ä¼˜åŒ–çš„ AI æœåŠ¡åè°ƒå™¨
 * 
 * èŒè´£ï¼š
 * - è¯·æ±‚é¢„å¤„ç†å’Œå¢å¼º
 * - æä¾›å•†è·¯ç”±å’Œåè°ƒ
 * - å“åº”åå¤„ç†å’Œæ ¼å¼åŒ–
 * - é”™è¯¯å¤„ç†å’Œå›é€€
 */

import { getGlobalConfig, type ModelProfile } from '../../utils/config.js'
import { getModelManager } from '../models/ModelManager.js'
import { getModelCapabilities } from '../models/modelCapabilities.js'
import { logError } from '../../utils/log.js'
// å·¥å…·ç®¡ç†æ¨¡å—
import { 
  getToolExecutionManager,
  setupToolEnvironment,
  type ToolAnalysisResult
} from './tools/index.js'
import { ToolExecutionStatus } from '../../tools/ToolOrchestrator.js'
import { getMessageLogger } from './messaging/MessageManager.js'
import { getInteractiveExecutionManager } from './interaction/InteractiveExecutionManager.js'

// å†…å®¹å¤„ç†æ¨¡å—
import {
  getContentProcessor,
  type ContentProcessingOptions
} from './content/index.js'

// æµå¼å¤„ç†æ¨¡å—
import {
  getStreamingManager,
  getResponseHandler,
  type StreamingManagerOptions,
  type ResponseHandlerOptions
} from './streaming/index.js'

// æ–°çš„å¼‚æ­¥æµå¼å¤„ç†æ¨¡å—
import {
  getAsyncStreamingManager,
  startAsyncStreaming,
  type StreamMessage
} from './streaming/AsyncStreamingManager.js'

// æä¾›å•†ç³»ç»Ÿ
import { 
  createProvider, 
  inferProviderFromModel
} from './providers/ProviderFactory.js'

// UI æ˜¾ç¤ºæ¨¡å—
import {
  getStreamingDisplay,
  displayMessageStream
} from '../../ui/components/StreamingDisplay.js'

// ä¿ç•™çš„æ ¸å¿ƒå¯¼å…¥
import type { 
  ContentBlock 
} from '../../types/UIMessage.js'
import { parseAIResponse } from './ResponseParser.js'
import { generateOptimizedSystemPrompt } from '../../tools/SystemPromptOptimizer.js'
import { addCostEntry } from '../CostTracker.js'
import { getContextManager, estimateTokens, ContextEntry } from '../ContextManager.js'
import { emitReminderEvent } from '../SystemReminderService.js'


// å…¼å®¹æ€§ç±»å‹ - ä¿æŒç°æœ‰æ¥å£ä¸å˜
export interface AIRequest {
  prompt: string
  systemPrompt?: string
  model?: string
  maxTokens?: number
  temperature?: number
  stream?: boolean
  onToken?: (chunk: string) => void
  allowedTools?: string[]
  enableToolCalls?: boolean
  enableSmartAnalysis?: boolean
  taskContext?: string
  autoGenerateSystemPrompt?: boolean
}

export interface AIResponse {
  content: string
  contentBlocks?: ContentBlock[]
  usage: {
    inputTokens: number
    outputTokens: number
  }
  cost: number
  duration: number
  model: string
  toolCalls?: ToolCall[]
  hasToolInteraction?: boolean
  toolCallsProcessed?: boolean
  streamingStats?: {
    duration: number
    tokenCount: number
    tokensPerSecond: number
    startTime: number
    endTime: number
  }
}

export interface ToolCall {
  toolName: string
  parameters: any
  callId: string
}

export interface AIToolExecutionResult {
  toolName: string
  callId: string
  result: string
  success: boolean
  error?: string
}

/**
 * WriteFlow AI æœåŠ¡ç±» - é‡æ„ç‰ˆæœ¬ï¼ˆåè°ƒå™¨æ¨¡å¼ï¼‰
 * 
 * ä»åŸæ¥çš„ 2000+ è¡Œç¼©å‡åˆ°çº¦ 400 è¡Œï¼Œä¸»è¦èŒè´£ï¼š
 * 1. è¯·æ±‚é¢„å¤„ç†å’Œå¢å¼º
 * 2. æä¾›å•†è·¯ç”±
 * 3. å“åº”åå¤„ç†
 * 4. é”™è¯¯å¤„ç†å’Œå›é€€
 */
export class WriteFlowAIService {
  private modelManager = getModelManager()
  private toolExecutionManager = getToolExecutionManager()
  private contentProcessor = getContentProcessor()
  private streamingManager = getStreamingManager()
  private responseHandler = getResponseHandler()
  private contextManager = getContextManager()
  private messageLogger = getMessageLogger()
  private interactiveManager = getInteractiveExecutionManager()
  
  /**
   * å¤„ç† AI è¯·æ±‚ï¼ˆæ”¯æŒæµå¼å’Œéæµå¼ï¼‰
   */
  async processRequest(request: AIRequest): Promise<AIResponse> {
    // ğŸŒŸ å¦‚æœå¯ç”¨äº†æµå¼å¤„ç†ï¼Œä½¿ç”¨ä¼˜åŒ–çš„æµå¼å®ç°
    if (request.stream && request.onToken) {
      console.log('ğŸŒŠ ä½¿ç”¨ä¼˜åŒ–æµå¼å¤„ç†...')
      
      // ğŸš€ ä¼˜åŒ–å­—ç¬¦ä¸²å¤„ç†ï¼šä½¿ç”¨æ•°ç»„æ‹¼æ¥å‡å°‘å†…å­˜å¼€é”€
      const contentChunks: string[] = []
      let finalResponse: AIResponse | null = null
      
      // ğŸš€ æµé‡æ§åˆ¶ï¼šé™åˆ¶å¤„ç†é¢‘ç‡ï¼Œé˜²æ­¢UIé˜»å¡
      let lastProcessTime = 0
      const MIN_PROCESS_INTERVAL = 8 // æœ€å°8msé—´éš”
      let pendingChunks: string[] = []
      
      // å¤„ç†æµå¼æ¶ˆæ¯
      for await (const message of this.processAsyncStreamingRequest(request)) {
        // æ£€æŸ¥æ˜¯å¦æ˜¯å­—ç¬¦çº§å¢é‡æ¶ˆæ¯
        if ((message as any).type === 'character_delta') {
          const delta = (message as any).delta
          if (delta && request.onToken) {
            contentChunks.push(delta)
            pendingChunks.push(delta)
            
            // æµé‡æ§åˆ¶ï¼šæ‰¹é‡å¤„ç†é¿å…é«˜é¢‘è°ƒç”¨
            const now = Date.now()
            if (now - lastProcessTime >= MIN_PROCESS_INTERVAL || pendingChunks.length >= 5) {
              request.onToken(pendingChunks.join(''))
              pendingChunks = []
              lastProcessTime = now
            }
          }
        } else if (message.type === 'progress') {
          // ğŸš€ å¤„ç†è¿›åº¦æ¶ˆæ¯ - å…³é”®ä¿®å¤ï¼šç¡®ä¿Progressæ¶ˆæ¯åˆ°è¾¾ç”¨æˆ·ç•Œé¢
          const progressMsg = message as any
          if (progressMsg.message && request.onToken) {
            // å¤„ç†å‰©ä½™çš„æŒ‚èµ·chunkså…ˆ
            if (pendingChunks.length > 0) {
              request.onToken(pendingChunks.join(''))
              pendingChunks = []
            }
            // ç«‹å³æ¨é€Progressæ¶ˆæ¯
            request.onToken(progressMsg.message)
            console.log('ğŸ“‹ [WriteFlowAIService] æ¨é€Progressæ¶ˆæ¯:', progressMsg.message.substring(0, 50))
          }
        } else if (message.type === 'ai_response') {
          // å¤„ç†å‰©ä½™çš„æŒ‚èµ·chunks
          if (pendingChunks.length > 0) {
            request.onToken(pendingChunks.join(''))
            pendingChunks = []
          }
          
          // æœ€ç»ˆå®Œæ•´å“åº”
          finalResponse = {
            content: (message as any).content || contentChunks.join(''),
            usage: (message as any).metadata ? {
              inputTokens: (message as any).metadata.tokensUsed || 0,
              outputTokens: (message as any).metadata.tokensUsed || 0
            } : { inputTokens: 0, outputTokens: 0 },
            cost: 0,
            duration: (message as any).metadata?.duration || 0,
            model: (message as any).metadata?.model || request.model || 'deepseek-chat'
          }
        }
      }
      
      // è¿”å›æœ€ç»ˆå“åº”
      return finalResponse || {
        content: contentChunks.join(''),
        usage: { inputTokens: 0, outputTokens: 0 },
        cost: 0,
        duration: 0,
        model: request.model || 'deepseek-chat'
      }
    }
    
    // éæµå¼å¤„ç†
    return this.processNonStreamingRequest(request)
  }

  /**
   * å¼‚æ­¥æµå¼å¤„ç† - é€šè¿‡ ProviderFactory ä¿æŒæä¾›å•†æ— å…³æ€§
   * æ”¯æŒå®æ—¶å·¥å…·æ‰§è¡Œæ˜¾ç¤ºï¼Œå…¼å®¹æ‰€æœ‰æä¾›å•†
   */
  async* processAsyncStreamingRequest(request: AIRequest): AsyncGenerator<StreamMessage, void, unknown> {
    const startTime = Date.now()
    
    try {
      // å‘é€å¼€å§‹æ¶ˆæ¯
      yield {
        type: 'system',
        level: 'info',
        message: 'WriteFlow AI å¼€å§‹å¤„ç†è¯·æ±‚',
        timestamp: startTime
      } as StreamMessage

      // é¢„å¤„ç†é˜¶æ®µ
      yield {
        type: 'progress',
        stage: 'preprocessing',
        message: 'é¢„å¤„ç†è¯·æ±‚å’Œåˆ†æå†…å®¹...',
        progress: 10
      } as StreamMessage

      const enhancedRequest = await this.enhanceRequest(request)
      
      // AI å“åº”é˜¶æ®µ
      yield {
        type: 'progress', 
        stage: 'ai_processing',
        message: 'å¼€å§‹å®æ—¶ AI å¤„ç†å’Œå·¥å…·æ‰§è¡Œ...',
        progress: 30
      } as StreamMessage

      // ğŸ¯ æ­£ç¡®æ¶æ„ï¼šé€šè¿‡ ProviderFactory è·å–æä¾›å•†
      const modelName = enhancedRequest.model || this.getDefaultModelName()
      const providerName = inferProviderFromModel(modelName)
      const provider = createProvider(providerName)
      
      // æ£€æŸ¥æä¾›å•†æ˜¯å¦æ”¯æŒ AsyncGenerator æµå¼æ¥å£
      if (typeof (provider as any).processAsyncStreamingRequest === 'function') {
        // ä½¿ç”¨æä¾›å•†çš„ AsyncGenerator æ¥å£
        for await (const message of (provider as any).processAsyncStreamingRequest(enhancedRequest)) {
          yield message
        }
      } else {
        // å›é€€åˆ°ä¼ ç»Ÿæµå¼å¤„ç†
        const response = await provider.processStreamingRequest(enhancedRequest, this.getModelProfile(modelName))
        
        yield {
          type: 'ai_response',
          content: response.content,
          isComplete: true,
          metadata: {
            model: response.model,
            tokensUsed: response.usage.outputTokens,
            duration: response.duration
          }
        } as StreamMessage
      }

      // å®Œæˆå¤„ç†
      const duration = Date.now() - startTime
      yield {
        type: 'system',
        level: 'info',
        message: `å¤„ç†å®Œæˆ (${duration}ms)`,
        timestamp: Date.now()
      } as StreamMessage

    } catch (error) {
      yield {
        type: 'error',
        message: `AIè¯·æ±‚å¤„ç†å¤±è´¥: ${error instanceof Error ? error.message : String(error)}`,
        error: error as Error,
        context: { request }
      } as StreamMessage
    }
  }

  /**
   * å¤„ç†æµå¼ AI è¯·æ±‚
   */
  async processStreamingRequest(request: AIRequest): Promise<AIResponse> {
    const streamRequest = { ...request, stream: true }
    return this.processNonStreamingRequest(streamRequest)
  }

  /**
   * å¤„ç†éæµå¼ AI è¯·æ±‚
   */
  async processNonStreamingRequest(request: AIRequest): Promise<AIResponse> {
    const startTime = Date.now()
    
    try {
      // é¢„å¤„ç†è¯·æ±‚ï¼šæ™ºèƒ½åˆ†æå’Œç³»ç»Ÿæç¤ºè¯å¢å¼º
      const enhancedRequest = await this.enhanceRequest(request)
      
      // è·å–æ¨¡å‹é…ç½®
      const modelName = enhancedRequest.model || this.getDefaultModelName()
      if (!modelName) {
        throw new Error('æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹é…ç½®')
      }
      
      // åˆ›å»ºæ¨¡å‹é…ç½®
      const modelProfile = this.findOrCreateModelProfile(modelName)
      if (!modelProfile) {
        throw new Error(`æ‰¾ä¸åˆ°æ¨¡å‹é…ç½®: ${modelName}`)
      }
      
      // ç¦»çº¿æ¨¡å¼å¤„ç†
      if (process.env.WRITEFLOW_AI_OFFLINE === 'true') {
        let offlineResponse = this.createOfflineResponse(enhancedRequest, startTime)
        
        // ç¦»çº¿æ¨¡å¼ä¹Ÿæ”¯æŒå·¥å…·äº¤äº’å¤„ç†
        if (enhancedRequest.enableToolCalls && enhancedRequest.allowedTools && enhancedRequest.allowedTools.length > 0) {
          // ä¿®æ”¹ç¦»çº¿å“åº”å†…å®¹ï¼ŒåŒ…å«ä¸€äº›å·¥å…·è°ƒç”¨ç¤ºä¾‹
          offlineResponse.content = this.createOfflineResponseWithToolCalls(enhancedRequest)
          offlineResponse = await this.processToolInteractions(offlineResponse, enhancedRequest)
          offlineResponse.hasToolInteraction = true
        }
        
        return offlineResponse
      }
      
      // è·å–æä¾›å•†
      const providerName = inferProviderFromModel(modelName)
      const provider = createProvider(providerName)
      
      // è½¬æ¢è¯·æ±‚æ ¼å¼
      const providerRequest = this.convertToProviderRequest(enhancedRequest)
      
      // è°ƒç”¨æä¾›å•†å¤„ç†è¯·æ±‚
      let response: AIResponse
      if (enhancedRequest.stream) {
        response = await provider.processStreamingRequest(enhancedRequest, modelProfile)
      } else {
        response = await provider.processRequest(enhancedRequest, modelProfile)
      }
      
      // è½¬æ¢å“åº”æ ¼å¼å¹¶åå¤„ç†
      let finalResponse = await this.convertFromProviderResponse(response, enhancedRequest)
      finalResponse.duration = Date.now() - startTime
      
      // å¦‚æœå¯ç”¨äº†å·¥å…·è°ƒç”¨ï¼Œå¤„ç†å·¥å…·äº¤äº’
      if (enhancedRequest.enableToolCalls && enhancedRequest.allowedTools && enhancedRequest.allowedTools.length > 0) {
        finalResponse = await this.processToolInteractions(finalResponse, enhancedRequest)
        finalResponse.hasToolInteraction = true
      }
      
      return finalResponse
      
    } catch (error) {
      return this.handleError(error, request, startTime)
    }
  }
  
  /**
   * å¢å¼ºè¯·æ±‚ï¼šè‡ªåŠ¨æ£€æµ‹å¹¶å¯ç”¨æ™ºèƒ½åˆ†æåŠŸèƒ½
   */
  private async enhanceRequest(request: AIRequest): Promise<AIRequest> {
    // ğŸ¯ æ–°å¢ï¼šä»»åŠ¡é¡ºåºéªŒè¯ï¼Œç¡®ä¿åˆ›ä½œä»»åŠ¡æŒ‰æ­£ç¡®é¡ºåºæ‰§è¡Œ
    await this.validateTaskOrder(request)
    
    // ä½¿ç”¨å·¥å…·æ‰§è¡Œç®¡ç†å™¨è®¾ç½®å·¥å…·ç¯å¢ƒ
    const enhanced = this.toolExecutionManager.setupToolEnvironment({
      prompt: request.prompt,
      enableToolCalls: request.enableToolCalls,
      allowedTools: request.allowedTools,
      enableSmartAnalysis: request.enableSmartAnalysis
    })
    
    // åˆå¹¶å…¶ä»–è¯·æ±‚å±æ€§
    const finalRequest = { ...request, ...enhanced }
    
    // è‡ªåŠ¨ç”Ÿæˆæˆ–å¢å¼ºç³»ç»Ÿæç¤ºè¯
    if (finalRequest.autoGenerateSystemPrompt !== false) {
      finalRequest.systemPrompt = await this.generateEnhancedSystemPrompt(finalRequest)
    }
    
    return finalRequest
  }
  
  
  /**
   * éªŒè¯ä»»åŠ¡æ‰§è¡Œé¡ºåºï¼Œç¡®ä¿åˆ›ä½œä»»åŠ¡æŒ‰æ­£ç¡®æµç¨‹æ‰§è¡Œ
   */
  private async validateTaskOrder(request: AIRequest): Promise<void> {
    try {
      // æ£€æµ‹æ˜¯å¦æ˜¯åˆ›ä½œç›¸å…³çš„è¯·æ±‚
      const isCreativeTask = this.isCreativeWritingTask(request.prompt)
      if (!isCreativeTask) {
        return // éåˆ›ä½œä»»åŠ¡æ— éœ€éªŒè¯é¡ºåº
      }

      // è·å–å½“å‰TODOçŠ¶æ€
      const todoManager = (globalThis as any).__writeflow_todo_manager__
      if (!todoManager) {
        return // æ²¡æœ‰TODOç®¡ç†å™¨æ—¶è·³è¿‡éªŒè¯
      }

      const currentTodos = await todoManager.getAllTodos()
      if (!currentTodos || currentTodos.length === 0) {
        return // æ²¡æœ‰å½“å‰ä»»åŠ¡æ—¶è·³è¿‡éªŒè¯
      }

      // å®šä¹‰åˆ›ä½œä»»åŠ¡çš„æ­£ç¡®é¡ºåº
      const creativeTaskOrder = [
        'æ¡†æ¶', 'å¤§çº²', 'ç»“æ„',
        'äººç‰©', 'è§’è‰²', 'è®¾å®š',
        'æ’°å†™', 'å†™ä½œ', 'åˆ›ä½œ',
        'å®Œå–„', 'ä¼˜åŒ–', 'æ¶¦è‰²',
      ]

      // æ£€æŸ¥å½“å‰è¯·æ±‚çš„ä»»åŠ¡ç±»å‹
      const currentTaskType = this.detectCreativeTaskType(request.prompt)
      if (!currentTaskType) {
        return // æ— æ³•è¯†åˆ«ä»»åŠ¡ç±»å‹æ—¶è·³è¿‡éªŒè¯
      }

      // è·å–å½“å‰è¿›è¡Œä¸­æˆ–å·²å®Œæˆçš„ä»»åŠ¡
      const activeTodos = currentTodos.filter((todo: any) => 
        todo.status === 'in_progress' || todo.status === 'completed'
      )

      // æ£€æŸ¥å‰ç½®ä»»åŠ¡æ˜¯å¦å®Œæˆ
      const currentTaskIndex = this.getTaskOrderIndex(currentTaskType, creativeTaskOrder)
      const hasUncompletedPrerequisites = this.checkPrerequisiteTasks(
        currentTaskIndex, 
        activeTodos, 
        creativeTaskOrder,
      )

      if (hasUncompletedPrerequisites) {
        console.warn(`âš ï¸ ä»»åŠ¡é¡ºåºéªŒè¯ï¼šå°è¯•æ‰§è¡Œ"${currentTaskType}"ï¼Œä½†å‰ç½®ä»»åŠ¡æœªå®Œæˆ`)
        
        // åœ¨ç³»ç»Ÿæç¤ºä¸­æ·»åŠ ä»»åŠ¡é¡ºåºæé†’ï¼Œè€Œä¸æ˜¯é˜»æ­¢æ‰§è¡Œ
        const orderReminder = `
        
ğŸ¯ ä»»åŠ¡æ‰§è¡Œé¡ºåºæé†’ï¼š
å½“å‰æ­£åœ¨å¤„ç†"${currentTaskType}"ä»»åŠ¡ï¼Œè¯·ç¡®ä¿ä¸¥æ ¼æŒ‰ä»¥ä¸‹é¡ºåºæ‰§è¡Œåˆ›ä½œä»»åŠ¡ï¼š
1. æ¡†æ¶è®¾è®¡ â†’ 2. äººç‰©è®¾å®š â†’ 3. å†…å®¹æ’°å†™ â†’ 4. å†…å®¹å®Œå–„

è¯·å…ˆå®Œæˆå‰ç½®æ­¥éª¤ï¼Œå†è¿›è¡Œå½“å‰ä»»åŠ¡ã€‚`
        
        request.systemPrompt = (request.systemPrompt || '') + orderReminder
      }

    } catch (error) {
      console.warn('ä»»åŠ¡é¡ºåºéªŒè¯å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ:', error)
      // éªŒè¯å¤±è´¥æ—¶ä¸é˜»æ­¢æ‰§è¡Œï¼Œåªè®°å½•è­¦å‘Š
    }
  }

  /**
   * æ£€æµ‹æ˜¯å¦ä¸ºåˆ›æ„å†™ä½œä»»åŠ¡
   */
  private isCreativeWritingTask(prompt: string): boolean {
    const creativeKeywords = [
      'å°è¯´', 'æ•…äº‹', 'åˆ›ä½œ', 'æ’°å†™', 'å†™ä½œ',
      'å‰§æœ¬', 'è¯—æ­Œ', 'æ–‡ç« ', 'å†…å®¹',
      'ä¸‰å›½', 'å†å²', 'ä¼ è®°', 'è®°å½•',
    ]
    
    return creativeKeywords.some(keyword => prompt.includes(keyword))
  }

  /**
   * æ£€æµ‹åˆ›ä½œä»»åŠ¡ç±»å‹
   */
  private detectCreativeTaskType(prompt: string): string | null {
    const taskPatterns = [
      { type: 'æ¡†æ¶', patterns: ['æ¡†æ¶', 'å¤§çº²', 'ç»“æ„', 'è§„åˆ’', 'è®¾è®¡æ¡†æ¶'] },
      { type: 'äººç‰©', patterns: ['äººç‰©', 'è§’è‰²', 'è®¾å®š', 'äººè®¾', 'è§’è‰²è®¾è®¡'] },
      { type: 'æ’°å†™', patterns: ['æ’°å†™', 'å†™ä½œ', 'åˆ›ä½œ', 'ç¼–å†™', 'ä¹¦å†™'] },
      { type: 'å®Œå–„', patterns: ['å®Œå–„', 'ä¼˜åŒ–', 'æ¶¦è‰²', 'ä¿®æ”¹', 'æ”¹è¿›'] },
    ]

    for (const { type, patterns } of taskPatterns) {
      if (patterns.some(pattern => prompt.includes(pattern))) {
        return type
      }
    }
    
    return null
  }

  /**
   * è·å–ä»»åŠ¡åœ¨é¡ºåºä¸­çš„ç´¢å¼•
   */
  private getTaskOrderIndex(taskType: string, orderArray: string[]): number {
    for (let i = 0; i < orderArray.length; i++) {
      if (orderArray[i] === taskType) {
        return i
      }
    }
    return -1
  }

  /**
   * æ£€æŸ¥å‰ç½®ä»»åŠ¡æ˜¯å¦å®Œæˆ
   */
  private checkPrerequisiteTasks(
    currentIndex: number, 
    activeTodos: any[], 
    orderArray: string[],
  ): boolean {
    if (currentIndex <= 0) {
      return false // ç¬¬ä¸€ä¸ªä»»åŠ¡æ— éœ€æ£€æŸ¥å‰ç½®æ¡ä»¶
    }

    // æ£€æŸ¥å‰ç½®ä»»åŠ¡æ˜¯å¦éƒ½å·²å®Œæˆ
    for (let i = 0; i < currentIndex; i++) {
      const prerequisiteType = orderArray[i]
      const hasCompletedPrerequisite = activeTodos.some((todo: any) =>
        todo.status === 'completed' && 
        this.todoContainsTaskType(todo.content, prerequisiteType)
      )
      
      if (!hasCompletedPrerequisite) {
        return true // å‘ç°æœªå®Œæˆçš„å‰ç½®ä»»åŠ¡
      }
    }
    
    return false // æ‰€æœ‰å‰ç½®ä»»åŠ¡éƒ½å·²å®Œæˆ
  }

  /**
   * æ£€æŸ¥TODOå†…å®¹æ˜¯å¦åŒ…å«æŒ‡å®šä»»åŠ¡ç±»å‹
   */
  private todoContainsTaskType(todoContent: string, taskType: string): boolean {
    const taskKeywords: Record<string, string[]> = {
      'æ¡†æ¶': ['æ¡†æ¶', 'å¤§çº²', 'ç»“æ„'],
      'äººç‰©': ['äººç‰©', 'è§’è‰²', 'è®¾å®š'],
      'æ’°å†™': ['æ’°å†™', 'å†™ä½œ', 'åˆ›ä½œ'],
      'å®Œå–„': ['å®Œå–„', 'ä¼˜åŒ–', 'æ¶¦è‰²'],
    }
    
    const keywords = taskKeywords[taskType] || [taskType]
    return keywords.some((keyword: string) => todoContent.includes(keyword))
  }

  /**
   * ç”Ÿæˆå¢å¼ºçš„ç³»ç»Ÿæç¤ºè¯
   */
  private async generateEnhancedSystemPrompt(request: AIRequest): Promise<string> {
    try {
      const optimizedPrompt = await generateOptimizedSystemPrompt({
        taskContext: request.taskContext,
        safeMode: false,
        compact: false
      })
      
      if (request.systemPrompt) {
        return `${optimizedPrompt}\n\n## ç”¨æˆ·è‡ªå®šä¹‰æŒ‡ä»¤\n${request.systemPrompt}`
      }
      
      return optimizedPrompt
    } catch (error) {
      console.warn('ç”Ÿæˆä¼˜åŒ–ç³»ç»Ÿæç¤ºè¯å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æç¤ºè¯:', error)
      return request.systemPrompt || 'ä½ æ˜¯ WriteFlow AI å†™ä½œåŠ©æ‰‹ï¼Œè¯·å¸®åŠ©ç”¨æˆ·å®Œæˆå„ç§å†™ä½œå’Œåˆ†æä»»åŠ¡ã€‚'
    }
  }
  
  /**
   * è·å–é»˜è®¤æ¨¡å‹åç§°
   */
  private getDefaultModelName(): string {
    // ä¼˜å…ˆä½¿ç”¨ AI_MODEL ç¯å¢ƒå˜é‡
    if (process.env.AI_MODEL) {
      return process.env.AI_MODEL
    }
    
    // å…¶æ¬¡ä½¿ç”¨ API_PROVIDER æ¨æ–­æ¨¡å‹
    const provider = process.env.API_PROVIDER
    switch (provider) {
      case 'deepseek': return 'deepseek-chat'
      case 'qwen3': return 'qwen-turbo'
      case 'glm4.5': return 'glm-4-flash'
      case 'anthropic': return 'claude-3-sonnet-20240229'
      case 'openai': return 'gpt-3.5-turbo'
      case 'kimi': return 'moonshot-v1-8k'
      default:
        // æ£€æŸ¥å¯ç”¨çš„ API Keyï¼Œæ™ºèƒ½é€‰æ‹©é»˜è®¤æ¨¡å‹
        if (process.env.DEEPSEEK_API_KEY) return 'deepseek-chat'
        if (process.env.ANTHROPIC_API_KEY) return 'claude-3-sonnet-20240229'
        if (process.env.OPENAI_API_KEY) return 'gpt-3.5-turbo'
        if (process.env.KIMI_API_KEY) return 'moonshot-v1-8k'
        if (process.env.GLM_API_KEY) return 'glm-4-flash'
        
        return 'deepseek-chat' // æœ€ç»ˆé»˜è®¤
    }
  }
  
  /**
   * æŸ¥æ‰¾æˆ–åˆ›å»ºæ¨¡å‹é…ç½®
   */
  private findOrCreateModelProfile(modelName: string): ModelProfile | null {
    // å…ˆå°è¯•ä»æ¨¡å‹ç®¡ç†å™¨ä¸­æŸ¥æ‰¾
    const profiles = this.modelManager.getAllProfiles()
    const existing = profiles.find(p => p.modelName === modelName || p.name === modelName)
    if (existing) {
      return existing
    }
    
    // åˆ›å»ºä¸´æ—¶æ¨¡å‹é…ç½®
    return this.createTempModelProfile(modelName)
  }
  
  /**
   * æ ¹æ®æ¨¡å‹åç§°åˆ›å»ºä¸´æ—¶çš„æ¨¡å‹é…ç½®
   */
  private createTempModelProfile(modelName: string): ModelProfile | null {
    const providerName = inferProviderFromModel(modelName)
    
    // æ ¹æ®æä¾›å•†åˆ›å»ºé…ç½®
    const providerConfigs = {
      anthropic: {
        baseURL: 'https://api.anthropic.com/v1/messages',
        envKeys: ['ANTHROPIC_API_KEY', 'CLAUDE_API_KEY']
      },
      deepseek: {
        baseURL: 'https://api.deepseek.com/v1/chat/completions',
        envKeys: ['DEEPSEEK_API_KEY']
      },
      openai: {
        baseURL: 'https://api.openai.com/v1/chat/completions',
        envKeys: ['OPENAI_API_KEY']
      },
      kimi: {
        baseURL: 'https://api.moonshot.cn/v1/chat/completions',
        envKeys: ['KIMI_API_KEY', 'MOONSHOT_API_KEY']
      },
      qwen: {
        baseURL: process.env.API_BASE_URL || 'https://dashscope.aliyuncs.com/compatible-mode/v1',
        envKeys: ['QWEN_API_KEY', 'DASHSCOPE_API_KEY']
      },
      glm: {
        baseURL: process.env.API_BASE_URL || 'https://open.bigmodel.cn/api/paas/v4',
        envKeys: ['GLM_API_KEY', 'ZHIPUAI_API_KEY']
      }
    }
    
    const config = providerConfigs[providerName as keyof typeof providerConfigs]
    if (!config) return null
    
    // è·å– API å¯†é’¥
    let apiKey = ''
    for (const envKey of config.envKeys) {
      const value = process.env[envKey]
      if (value) {
        apiKey = value
        break
      }
    }
    
    if (!apiKey) {
      console.warn(`æ‰¾ä¸åˆ° ${providerName} çš„ API å¯†é’¥`)
      return null
    }
    
    return {
      name: `temp-${modelName}`,
      provider: providerName as any,
      modelName: modelName,
      baseURL: config.baseURL,
      apiKey: apiKey,
      maxTokens: 4000,
      contextLength: 128000,
      isActive: true
    }
  }
  
  /**
   * è½¬æ¢åˆ°æä¾›å•†è¯·æ±‚æ ¼å¼ (æš‚æ—¶ç›´æ¥è¿”å›)
   */
  private convertToProviderRequest(request: AIRequest): AIRequest {
    return request
  }
  
  /**
   * è½¬æ¢æä¾›å•†å“åº”æ ¼å¼ï¼Œä½¿ç”¨å†…å®¹å¤„ç†å™¨
   */
  private async convertFromProviderResponse(response: AIResponse, originalRequest: AIRequest): Promise<AIResponse> {
    // ä½¿ç”¨å†…å®¹å¤„ç†å™¨å¤„ç†å“åº”å†…å®¹
    const processed = await this.contentProcessor.processAIResponse(response.content, {
      enableCollapsible: true,
      enableAnalysis: false,
      parseMarkdown: true,
      enhanceFormatting: false
    })
    
    return {
      content: response.content,
      contentBlocks: processed.contentBlocks,
      usage: response.usage,
      cost: response.cost,
      duration: response.duration,
      model: response.model,
      streamingStats: response.streamingStats
    }
  }
  
  /**
   * å¤„ç†å·¥å…·äº¤äº’ - é›†æˆæ¸è¿›å¼å±•ç¤º
   */
  private async processToolInteractions(response: AIResponse, request: AIRequest): Promise<AIResponse> {
    const maxIterations = 10
    let iterationCount = 0
    let currentContent = response.content

    while (iterationCount < maxIterations) {
      // æ£€æµ‹å·¥å…·è°ƒç”¨
      const toolCalls = this.detectToolCalls(currentContent, request.allowedTools!)
      
      if (toolCalls.length === 0) {
        // æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç»“æŸå¤„ç†
        break
      }

      this.messageLogger.systemInfo(`æ£€æµ‹åˆ° ${toolCalls.length} ä¸ªå·¥å…·è°ƒç”¨`)
      
      // åˆ›å»ºäº¤äº’å¼æ‰§è¡Œè®¡åˆ’
      const executionPlan = this.interactiveManager.createExecutionPlan(
        `AIè¯·æ±‚å·¥å…·æ‰§è¡Œ - ç¬¬${iterationCount + 1}è½®`,
        toolCalls.map(call => ({
          toolName: call.toolName,
          parameters: call.parameters
        })),
        {
          requireConfirmation: false,  // AIè‡ªåŠ¨æ‰§è¡Œæ—¶ä¸éœ€è¦ç¡®è®¤
          allowInterruption: false,    // æ‰¹é‡æ‰§è¡Œæ—¶ä¸å…è®¸ä¸­æ–­ 
          showPreview: true,          // æ˜¾ç¤ºæ‰§è¡Œé¢„è§ˆ
          batchMode: true             // æ‰¹é‡æ¨¡å¼
        }
      )
      
      // å¼€å§‹äº¤äº’å¼æ‰§è¡Œ
      const session = await this.interactiveManager.startInteractiveExecution(executionPlan, {
        requireConfirmation: false,
        allowInterruption: false,
        showPreview: true,
        batchMode: true
      })
      
      // å°†äº¤äº’å¼æ‰§è¡Œç»“æœè½¬æ¢ä¸ºå·¥å…·ç»“æœ
      const toolResults: any[] = session.results.map(result => ({
        toolName: result.toolName,
        callId: `interactive_${result.executionId}`,
        result: result.result,
        success: result.status === ToolExecutionStatus.COMPLETED,
        error: result.error?.message
      }))

      // å¦‚æœæ²¡æœ‰ç»“æœï¼Œå›é€€åˆ°ç›´æ¥æ‰§è¡Œ
      if (toolResults.length === 0) {
        for (const toolCall of toolCalls) {
          try {
            const result = await this.toolExecutionManager.executeToolCall(
              toolCall.toolName,
              toolCall.parameters,
              { 
                requestId: 'ai-service',
                userId: 'ai-user'
              }
            )
            
            toolResults.push({
              toolName: toolCall.toolName,
              callId: toolCall.callId,
              result: result.result,
              success: result.status === ToolExecutionStatus.COMPLETED,
              error: result.error?.message
            })
          } catch (error) {
            // å¯¹äº TODO ç›¸å…³å·¥å…·ï¼Œå°è¯•ä½¿ç”¨ legacy æ‰§è¡Œæ–¹å¼
            if (['todo_write', 'todo_read', 'exit_plan_mode'].includes(toolCall.toolName)) {
              try {
                const legacyResult = await this.executeLegacyTool(toolCall.toolName, toolCall.parameters)
                if (legacyResult) {
                  toolResults.push({
                    toolName: toolCall.toolName,
                    callId: toolCall.callId,
                    result: legacyResult.result,
                    success: legacyResult.success,
                    error: legacyResult.error
                  })
                  continue
                }
              } catch (legacyError) {
                // Legacy æ‰§è¡Œä¹Ÿå¤±è´¥äº†ï¼Œè®°å½•é”™è¯¯
                this.messageLogger.systemError(`Legacy tool execution failed for ${toolCall.toolName}: ${legacyError}`)
              }
            }
            
            // æ ‡å‡†é”™è¯¯å¤„ç†
            toolResults.push({
              toolName: toolCall.toolName,
              callId: toolCall.callId,
              result: '',
              success: false,
              error: error instanceof Error ? error.message : String(error)
            })
          }
        }
      }

      // æ›´æ–°å†…å®¹ï¼Œç§»é™¤å·²å¤„ç†çš„å·¥å…·è°ƒç”¨
      currentContent = this.updateContentWithToolResults(currentContent, toolResults)
      
      iterationCount++
      
      // å¦‚æœæ‰€æœ‰å·¥å…·éƒ½å¤±è´¥äº†ï¼Œåœæ­¢è¿­ä»£
      if (toolResults.every(result => !result.success)) {
        this.messageLogger.systemWarning('æ‰€æœ‰å·¥å…·è°ƒç”¨éƒ½å¤±è´¥äº†ï¼Œåœæ­¢å¤„ç†')
        break
      }
    }

    return {
      ...response,
      content: currentContent,
      toolCallsProcessed: iterationCount > 0
    }
  }

  /**
   * æ£€æµ‹å·¥å…·è°ƒç”¨
   */
  private detectToolCalls(content: string, allowedTools: string[]): any[] {
    const toolCalls: any[] = []
    
    // æ£€æµ‹æ ¼å¼ï¼šWrite("filename", "content")
    const toolCallPattern = /(\w+)\s*\(([^)]+)\)/g
    let match

    while ((match = toolCallPattern.exec(content)) !== null) {
      const [fullMatch, toolName, argsStr] = match
      
      if (allowedTools.includes(toolName)) {
        try {
          // å°è¯•è§£æå‚æ•°
          const parameters = this.parseToolArguments(argsStr)
          
          toolCalls.push({
            toolName,
            callId: `call_${Date.now()}_${Math.random().toString(36).slice(2)}`,
            parameters,
            fullMatch
          })
        } catch (error) {
          this.messageLogger.systemWarning(`è§£æå·¥å…·è°ƒç”¨å‚æ•°å¤±è´¥ ${toolName}: ${error}`)
        }
      }
    }

    return toolCalls
  }

  /**
   * è§£æå·¥å…·å‚æ•°
   */
  private parseToolArguments(argsStr: string): any {
    // ç®€å•çš„å‚æ•°è§£æ - æ”¯æŒå­—ç¬¦ä¸²å’ŒåŸºæœ¬ç±»å‹
    const args = argsStr.split(',').map(arg => arg.trim().replace(/['"]/g, ''))
    
    // æ ¹æ®å‚æ•°æ•°é‡ç¡®å®šå‚æ•°ç»“æ„
    if (args.length === 1) {
      return { input: args[0] }
    } else if (args.length === 2) {
      return { path: args[0], content: args[1] }
    } else {
      return { args }
    }
  }

  /**
   * æ›´æ–°å†…å®¹ï¼Œå°†å·¥å…·è°ƒç”¨æ›¿æ¢ä¸ºç»“æœ
   */
  private updateContentWithToolResults(content: string, results: any[]): string {
    let updatedContent = content

    for (const result of results) {
      if (result.success) {
        // åœ¨å·¥å…·è°ƒç”¨ä½ç½®æ˜¾ç¤ºæ‰§è¡Œç»“æœ
        const toolCallPattern = new RegExp(`${result.toolName}\\s*\\([^)]+\\)`, 'gi')
        updatedContent = updatedContent.replace(toolCallPattern, (match) => {
          const resultPreview = result.result ? result.result.slice(0, 100) : ''
          return `${match}\nâœ… [å·¥å…·æ‰§è¡Œå®Œæˆ] ${resultPreview}${result.result?.length > 100 ? '...' : ''}`
        })
      } else {
        // æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
        const toolCallPattern = new RegExp(`${result.toolName}\\s*\\([^)]+\\)`, 'gi')
        updatedContent = updatedContent.replace(toolCallPattern, (match) => {
          return `${match}\nâŒ [å·¥å…·æ‰§è¡Œå¤±è´¥] ${result.error || 'æœªçŸ¥é”™è¯¯'}`
        })
      }
    }

    return updatedContent
  }

  /**
   * åˆ›å»ºç¦»çº¿æ¨¡å¼å“åº”
   */
  private createOfflineResponse(request: AIRequest, startTime: number): AIResponse {
    const content = `ã€ç¦»çº¿æ¨¡å¼ã€‘æ— æ³•è®¿é—®å¤–éƒ¨æ¨¡å‹ï¼Œå·²è¿”å›æ¨¡æ‹Ÿå›å¤ã€‚\n\nè¦ç‚¹: ${request.prompt.slice(0, 120)}${request.prompt.length > 120 ? '...' : ''}`
    const parsedResponse = parseAIResponse(content)
    
    return {
      content,
      contentBlocks: parsedResponse.content,
      usage: { inputTokens: 0, outputTokens: content.length },
      cost: 0,
      duration: Date.now() - startTime,
      model: 'offline-mock'
    }
  }
  
  /**
   * åˆ›å»ºåŒ…å«å·¥å…·è°ƒç”¨çš„ç¦»çº¿å“åº”
   */
  private createOfflineResponseWithToolCalls(request: AIRequest): string {
    const prompt = request.prompt.toLowerCase()
    
    // æ ¹æ®ç”¨æˆ·è¯·æ±‚æ™ºèƒ½ç”Ÿæˆå·¥å…·è°ƒç”¨ç¤ºä¾‹
    if (prompt.includes('è¯»å–') || prompt.includes('read')) {
      return `ã€ç¦»çº¿æ¨¡å¼æ¼”ç¤ºã€‘æ­£åœ¨æ‰§è¡Œæ‚¨çš„è¯·æ±‚...

æˆ‘å°†ä½¿ç”¨ä»¥ä¸‹å·¥å…·æ¥å®Œæˆæ‚¨çš„ä»»åŠ¡ï¼š

Read("README.md")

è®©æˆ‘è¯»å–README.mdæ–‡ä»¶æ¥äº†è§£é¡¹ç›®ä¿¡æ¯ã€‚`
    }
    
    if (prompt.includes('æœç´¢') || prompt.includes('æŸ¥æ‰¾') || prompt.includes('grep')) {
      return `ã€ç¦»çº¿æ¨¡å¼æ¼”ç¤ºã€‘æ­£åœ¨æ‰§è¡Œæœç´¢ä»»åŠ¡...

Grep("function", "*.ts")

è®©æˆ‘åœ¨TypeScriptæ–‡ä»¶ä¸­æœç´¢å‡½æ•°å®šä¹‰ã€‚`
    }
    
    if (prompt.includes('å†™å…¥') || prompt.includes('åˆ›å»ºæ–‡ä»¶') || prompt.includes('write')) {
      return `ã€ç¦»çº¿æ¨¡å¼æ¼”ç¤ºã€‘æ­£åœ¨åˆ›å»ºæ–‡ä»¶...

Write("example.md", "# ç¤ºä¾‹æ–‡ä»¶\\nè¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹æ–‡ä»¶çš„å†…å®¹ã€‚")

è®©æˆ‘åˆ›å»ºä¸€ä¸ªç¤ºä¾‹æ–‡ä»¶ã€‚`
    }
    
    // é»˜è®¤å“åº” - åŒ…å«å¤šä¸ªå·¥å…·è°ƒç”¨ç¤ºä¾‹
    return `ã€ç¦»çº¿æ¨¡å¼æ¼”ç¤ºã€‘æ­£åœ¨åˆ†ææ‚¨çš„éœ€æ±‚å¹¶æ‰§è¡Œç›¸åº”çš„å·¥å…·...

é¦–å…ˆè®©æˆ‘è¯»å–é¡¹ç›®ä¿¡æ¯ï¼š
Read("README.md")

ç„¶åæœç´¢ç›¸å…³æ–‡ä»¶ï¼š
Grep("${request.prompt.slice(0, 50)}", "**/*.md")

æœ€ååˆ›å»ºåˆ†æç»“æœï¼š
Write("analysis.md", "# åˆ†æç»“æœ\\nåŸºäºæ‚¨çš„è¯·æ±‚è¿›è¡Œçš„åˆ†æ...")

è¿™æ ·æˆ‘å¯ä»¥ä¸ºæ‚¨æä¾›å…¨é¢çš„åˆ†æå’Œå¤„ç†ã€‚`
  }

  /**
   * é”™è¯¯å¤„ç†
   */
  private handleError(error: any, request: AIRequest, startTime: number): AIResponse {
    logError('AI è¯·æ±‚å¤„ç†å¤±è´¥', error)
    
    const hint = `\næç¤º: \n- è¯·æ£€æŸ¥ç½‘ç»œè¿é€šæ€§æˆ–ä»£ç†è®¾ç½®\n- å¦‚éœ€ç¦»çº¿æ¼”ç¤º: export WRITEFLOW_AI_OFFLINE=true\n- æˆ–æ­£ç¡®è®¾ç½® API_PROVIDER/AI_MODEL åŠå¯¹åº”çš„ *API_KEY ç¯å¢ƒå˜é‡\n- å¯é€‰ API_BASE_URL è¦†ç›–é»˜è®¤ç½‘å…³`
    
    return {
      content: `å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}${hint}`,
      usage: { inputTokens: 0, outputTokens: 0 },
      cost: 0,
      duration: Date.now() - startTime,
      model: request.model || 'unknown'
    }
  }

  /**
   * æ‰§è¡Œä¼ ç»Ÿå·¥å…·è°ƒç”¨ - ç‰¹åˆ«å¤„ç† TODO å·¥å…·
   * ä» backup ç‰ˆæœ¬æ¢å¤ï¼Œç”¨äºæ”¯æŒ AI è°ƒç”¨ TODO å·¥å…·
   */
  private async executeLegacyTool(toolName: string, params: any): Promise<{ result: string; success: boolean; error?: string } | null> {
    try {
      // ç»Ÿä¸€ä¼šè¯ IDï¼Œç¡®ä¿ä¸ UI/CLI ä½¿ç”¨åŒä¸€ä¸ª Todo å­˜å‚¨
      const sessionId = process.env.WRITEFLOW_SESSION_ID
      const { TodoManager } = await import('../../tools/TodoManager.js')
      const sharedManager = new TodoManager(sessionId)

      if (toolName === 'todo_write') {
        const { TodoWriteTool } = await import('../../tools/writing/TodoWriteTool.js')
        const tool = new TodoWriteTool(sharedManager)
        const res = await tool.execute(params, { 
          agentId: 'ai-service', 
          abortController: new AbortController(), 
          options: { verbose: false } 
        })
        return { result: res.content || '', success: res.success, error: res.success ? undefined : (res as any).error }
      }
      if (toolName === 'todo_read') {
        const { TodoReadTool } = await import('../../tools/writing/TodoReadTool.js')
        const tool = new TodoReadTool(sharedManager)
        const res = await tool.execute(params, { 
          agentId: 'ai-service', 
          abortController: new AbortController(), 
          options: { verbose: false } 
        })
        return { result: res.content || '', success: res.success, error: res.success ? undefined : (res as any).error }
      }
      if (toolName === 'exit_plan_mode') {
        // ç®€åŒ–å¤„ç†ï¼šè¿”å›å›ºå®šæ¶ˆæ¯ï¼Œäº¤ç”±ä¸Šå±‚è§£æ
        return { result: 'å·²é€€å‡ºè®¡åˆ’æ¨¡å¼', success: true }
      }
      return null
    } catch (error) {
      return { result: '', success: false, error: (error as Error).message }
    }
  }
  
  /**
   * è·å–æ¨¡å‹é…ç½® - ä¿æŒç°æœ‰å…¼å®¹æ€§
   */
  private getModelProfile(modelName: string): ModelProfile {
    const profile = this.findOrCreateModelProfile(modelName)
    if (!profile) {
      throw new Error(`æ— æ³•åˆ›å»ºæ¨¡å‹é…ç½®: ${modelName}`)
    }
    return profile
  }
  
}

// å…¨å±€æœåŠ¡å®ä¾‹ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
let globalAIService: WriteFlowAIService | null = null

/**
 * è·å–å…¨å±€ WriteFlow AI æœåŠ¡å®ä¾‹
 */
export function getWriteFlowAIService(): WriteFlowAIService {
  if (!globalAIService) {
    globalAIService = new WriteFlowAIService()
  }
  return globalAIService
}

/**
 * å¯¼å‡ºé»˜è®¤å®ä¾‹ï¼ˆå…¼å®¹æ€§ï¼‰
 */
export const writeFlowAIService = getWriteFlowAIService()