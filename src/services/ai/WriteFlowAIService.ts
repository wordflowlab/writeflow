/**
 * WriteFlow AI æœåŠ¡
 * ä¸“ä¸ºå†™ä½œåœºæ™¯ä¼˜åŒ–çš„ AI æœåŠ¡
 */

import { getGlobalConfig, ModelProfile } from '../../utils/config.js'
import { getModelManager } from '../models/ModelManager.js'
import { getModelCapabilities } from '../models/modelCapabilities.js'
import { logError } from '../../utils/log.js'
import { 
  getTool, 
  getToolOrchestrator, 
  getPermissionManager,
  getAvailableTools,
  executeToolQuick,
  ToolExecutionStatus,
  type ToolExecutionResult,
  type WriteFlowTool
} from '../../tools/index.js'
import { AgentContext } from '../../types/agent.js'
import { ToolUseContext } from '../../Tool.js'
import { getStreamingService, StreamingService, StreamingRequest } from '../streaming/StreamingService.js'
import { getResponseStateManager } from '../streaming/ResponseStateManager.js'
import { getProviderAdapter } from './providers/index.js'
import { emitReminderEvent } from '../SystemReminderService.js'
import { startStreamingProgress, stopStreamingProgress } from '../streaming/ProgressIndicator.js'
import { getOutputFormatter } from '../../ui/utils/outputFormatter.js'
import { parseAIResponse, parseStreamingChunk, type ParsedResponse } from './ResponseParser.js'
import type { ContentBlock } from '../../types/UIMessage.js'

export interface AIRequest {
  prompt: string
  systemPrompt?: string
  model?: string
  maxTokens?: number
  temperature?: number
  stream?: boolean
  onToken?: (chunk: string) => void
  allowedTools?: string[]
  enableToolCalls?: boolean
}

export interface AIResponse {
  content: string
  contentBlocks?: ContentBlock[]  // æ–°å¢ï¼šç»“æ„åŒ–å†…å®¹å—
  usage: {
    inputTokens: number
    outputTokens: number
  }
  cost: number
  duration: number
  model: string
  toolCalls?: ToolCall[]
  hasToolInteraction?: boolean
  streamingStats?: {
    duration: number
    tokenCount: number
    tokensPerSecond: number
    startTime: number
    endTime: number
  }
}

export interface ToolCall {
  toolName: string
  parameters: any
  callId: string
}

export interface AIToolExecutionResult {
  toolName: string
  callId: string
  result: string
  success: boolean
  error?: string
}

/**
 * WriteFlow AI æœåŠ¡ç±» - é›†æˆå¢å¼ºå·¥å…·ç³»ç»Ÿ
 */
export class WriteFlowAIService {
  private modelManager = getModelManager()
  private toolOrchestrator = getToolOrchestrator()
  private permissionManager = getPermissionManager()
  private providerAdapter = getProviderAdapter(undefined)
  
  /**
   * å¤„ç† AI è¯·æ±‚ï¼ˆæ”¯æŒæµå¼å’Œéæµå¼ï¼‰
   */
  async processRequest(request: AIRequest): Promise<AIResponse> {
    // å¦‚æœè¯·æ±‚æµå¼å¤„ç†ï¼Œå§”æ‰˜ç»™æµå¼æœåŠ¡
    if (request.stream) {
      return this.processStreamingRequest(request)
    }
    
    return this.processNonStreamingRequest(request)
  }

  /**
   * è¿‡æ»¤ DeepSeek ç­‰æ¨¡å‹åœ¨æ–‡æœ¬ä¸­å†…è”æš´éœ²çš„å·¥å…·æ ‡è®°
   * æ¸…ç†å½¢å¦‚ <ï½œtoolâ–callsâ–beginï½œ> ... <ï½œtoolâ–callsâ–endï½œ> ä»¥åŠå•ä¸ª <ï½œtoolâ–...ï½œ>
   */
  private sanitizeLLMArtifacts(text: string | undefined): string {
    return this.providerAdapter.sanitizeText(text || '')
  }

  /**
   * ä»æ–‡æœ¬ä¸­æå– DeepSeek çš„å†…è”å·¥å…·è°ƒç”¨ï¼Œè¿”å›æ¸…ç†åçš„æ–‡æœ¬ä¸è§£æå‡ºçš„è°ƒç”¨
   */
  private extractInlineToolCalls(text: string) {
    return this.providerAdapter.extractInlineToolCalls(text)
  }
  
  /**
   * å¤„ç†æµå¼ AI è¯·æ±‚ - ç®€åŒ–ç‰ˆæœ¬ï¼Œç›´æ¥å¤„ç†è€Œä¸ä¾èµ–å¤æ‚çš„ StreamingService
   */
  async processStreamingRequest(request: AIRequest): Promise<AIResponse> {
    const startTime = Date.now()
    
    try {
      // ç¦»çº¿æ¨¡å¼ä¸‹ç›´æ¥å›é€€åˆ°éæµå¼å¤„ç†
      if (process.env.WRITEFLOW_AI_OFFLINE === 'true') {
        return this.processNonStreamingRequest({ ...request, stream: false })
      }
      
      // ä»è¯·æ±‚æˆ–ç¯å¢ƒå˜é‡è·å–æ¨¡å‹åç§°
      const modelName = request.model || process.env.AI_MODEL || this.getDefaultModelName()
      if (!modelName) {
        console.warn('æ²¡æœ‰æŒ‡å®šæ¨¡å‹ï¼Œå›é€€åˆ°éæµå¼å¤„ç†')
        return this.processNonStreamingRequest({ ...request, stream: false })
      }
      
      // æ ¹æ®æ¨¡å‹åç§°åˆ›å»ºä¸´æ—¶çš„æ¨¡å‹é…ç½®
      const modelProfile = this.createTempModelProfile(modelName)
      if (!modelProfile) {
        console.warn(`ä¸æ”¯æŒçš„æ¨¡å‹: ${modelName}ï¼Œå›é€€åˆ°éæµå¼å¤„ç†`)
        return this.processNonStreamingRequest({ ...request, stream: false })
      }
      
      // æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒæµå¼
      const capabilities = getModelCapabilities(modelName)
      if (!capabilities.supportsStreaming) {
        console.warn(`æ¨¡å‹ ${modelName} ä¸æ”¯æŒæµå¼ï¼Œå›é€€åˆ°éæµå¼å¤„ç†`)
        return this.processNonStreamingRequest({ ...request, stream: false })
      }
      
      // ç›´æ¥è°ƒç”¨å¯¹åº”çš„ API è¿›è¡Œæµå¼å¤„ç†
      this.providerAdapter = getProviderAdapter(modelProfile.provider)
      switch (modelProfile.provider) {
        case 'deepseek':
          return await this.callDeepSeekAPI(modelProfile, request)
        case 'anthropic':
        case 'bigdream':
          return await this.callAnthropicAPI(modelProfile, request)
        case 'openai':
        case "custom-openai":
        case "custom":
          return await this.callOpenAIAPI(modelProfile, request)
        case 'kimi':
          return await this.callKimiAPI(modelProfile, request)
        default:
          console.warn(`ä¸æ”¯æŒæµå¼çš„æä¾›å•†: ${modelProfile.provider}ï¼Œå›é€€åˆ°éæµå¼å¤„ç†`)
          return this.processNonStreamingRequest({ ...request, stream: false })
      }
      
    } catch (error) {
      logError('æµå¼ AI è¯·æ±‚å¤„ç†å¤±è´¥', error)
      
      // æ ‡è®°æµå¼å“åº”é”™è¯¯ï¼ˆå¦‚æœæœ‰æ´»è·ƒçš„æµå¼ä¼šè¯ï¼‰
      const responseManager = getResponseStateManager()
      const activeStats = responseManager.getActiveStreamingStats()
      if (activeStats.activeStreams > 0) {
        // æ‰¾åˆ°å¯èƒ½çš„æµå¼ä¼šè¯å¹¶æ ‡è®°é”™è¯¯
        console.warn(`å‘ç° ${activeStats.activeStreams} ä¸ªæ´»è·ƒæµå¼ä¼šè¯ï¼Œæ ‡è®°ä¸ºé”™è¯¯çŠ¶æ€`)
      }
      
      // å›é€€åˆ°éæµå¼å¤„ç†ï¼Œæ˜ç¡®è®¾ç½® stream: false é˜²æ­¢é€’å½’
      console.warn('æµå¼å¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°éæµå¼å¤„ç†')
      return this.processNonStreamingRequest({ ...request, stream: false })
    }
  }
  
  /**
   * å¤„ç†éæµå¼ AI è¯·æ±‚
   */
  async processNonStreamingRequest(request: AIRequest): Promise<AIResponse> {
    const startTime = Date.now()

    try {
      // ç¦»çº¿/é™çº§æ¨¡å¼ï¼ˆæœ¬åœ°æ— ç½‘æˆ–æ—  Key æ—¶å¯ç”¨ï¼‰
      if (process.env.WRITEFLOW_AI_OFFLINE === 'true') {
        const content = `ã€ç¦»çº¿æ¨¡å¼ã€‘æ— æ³•è®¿é—®å¤–éƒ¨æ¨¡å‹ï¼Œå·²è¿”å›æ¨¡æ‹Ÿå›å¤ã€‚\n\nè¦ç‚¹: ${request.prompt.slice(0, 120)}${request.prompt.length > 120 ? '...' : ''}`
        const parsedResponse = parseAIResponse(content)
        return {
          content,
          contentBlocks: parsedResponse.content,
          usage: { inputTokens: 0, outputTokens: content.length },
          cost: 0,
          duration: Date.now() - startTime,
          model: 'offline-mock'
        }
      }

      // è·å–æ¨¡å‹é…ç½®
      const modelName = request.model || this.modelManager.getMainAgentModel()
      if (!modelName) {
        throw new Error('æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹é…ç½®')
      }

      const modelProfile = this.findModelProfile(modelName)
      if (!modelProfile) {
        throw new Error(`æ‰¾ä¸åˆ°æ¨¡å‹é…ç½®: ${modelName}`)
      }
      this.providerAdapter = getProviderAdapter(modelProfile.provider)

      // æ ¹æ®æä¾›å•†è°ƒç”¨ç›¸åº”çš„ AI æœåŠ¡
      let response: AIResponse

      switch (modelProfile.provider) {
        case 'anthropic':
        case 'bigdream':
          response = await this.callAnthropicAPI(modelProfile, request)
          break
        case 'deepseek':
          response = await this.callDeepSeekAPI(modelProfile, request)
          break
        case 'openai':
        case 'custom-openai':
          response = await this.callOpenAIAPI(modelProfile, request)
          break
        case 'custom':
          // å¯¹äºå®Œå…¨è‡ªå®šä¹‰æä¾›å•†ï¼Œæš‚æ—¶å›é€€åˆ° OpenAIå…¼å®¹æ ¼å¼
          // æœªæ¥å¯ä»¥æ‰©å±•æ”¯æŒå®Œå…¨è‡ªå®šä¹‰çš„APIæ ¼å¼
          response = await this.callOpenAIAPI(modelProfile, request)
          break
        case 'kimi':
          response = await this.callKimiAPI(modelProfile, request)
          break
        default:
          throw new Error(`ä¸æ”¯æŒçš„æä¾›å•†: ${modelProfile.provider}`)
      }

      // è®¡ç®—æŒç»­æ—¶é—´
      response.duration = Date.now() - startTime

      // å¦‚æœå¯ç”¨äº†å·¥å…·è°ƒç”¨ï¼Œå¤„ç†å·¥å…·äº¤äº’
      if (request.enableToolCalls && request.allowedTools && request.allowedTools.length > 0) {
        response = await this.processToolInteractions(response, request)
      }

      // æ¸…ç†å¯èƒ½æ®‹ç•™çš„å†…è”æ ‡è®°ï¼ˆæŒ‰ provider é€‚é…å™¨ï¼‰
      response.content = this.providerAdapter.sanitizeText(response.content)
      return response

    } catch (error) {
      logError('AI è¯·æ±‚å¤„ç†å¤±è´¥', error)

      const hint = `\næç¤º: \n- è¯·æ£€æŸ¥ç½‘ç»œè¿é€šæ€§æˆ–ä»£ç†è®¾ç½®\n- å¦‚éœ€ç¦»çº¿æ¼”ç¤º: export WRITEFLOW_AI_OFFLINE=true\n- æˆ–æ­£ç¡®è®¾ç½® API_PROVIDER/AI_MODEL åŠå¯¹åº”çš„ *API_KEY ç¯å¢ƒå˜é‡\n- å¯é€‰ API_BASE_URL è¦†ç›–é»˜è®¤ç½‘å…³`
      return {
        content: `å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}${hint}`,
        usage: { inputTokens: 0, outputTokens: 0 },
        cost: 0,
        duration: Date.now() - startTime,
        model: request.model || 'unknown'
      }
    }
  }
  
  /**
   * è°ƒç”¨ Anthropic API
   */
  private async callAnthropicAPI(profile: ModelProfile, request: AIRequest): Promise<AIResponse> {
    const apiKey = this.getAPIKey(profile, ['ANTHROPIC_API_KEY', 'CLAUDE_API_KEY'])
    if (!apiKey) {
      throw new Error('ç¼ºå°‘ Anthropic API å¯†é’¥')
    }
    
    const url = profile.baseURL || 'https://api.anthropic.com/v1/messages'
    
    const payload: any = {
      model: profile.modelName,
      max_tokens: request.maxTokens || profile.maxTokens,
      temperature: request.temperature || 0.3,
      messages: [
        {
          role: 'user',
          content: request.prompt
        }
      ],
      ...(request.systemPrompt && { system: request.systemPrompt })
    }
    // Anthropic ä¹Ÿæ”¯æŒæµå¼
    if (request.stream) {
      payload.stream = true
    }

    const response = await fetch(url, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'x-api-key': apiKey,
        'anthropic-version': '2023-06-01'
      },
      body: JSON.stringify(payload)
    })
    
    if (!response.ok) {
      const errorText = await response.text()
      throw new Error(`Anthropic API é”™è¯¯: ${response.status} - ${errorText}`)
    }
    
    if (request.stream) {
      return await this.handleAnthropicStreamingResponse(response, profile, request)
    }

    const data = await response.json()
    
    const rawContent = data.content?.[0]?.text || 'æ— å“åº”å†…å®¹'
    const parsedResponse = parseAIResponse(rawContent)
    
    return {
      content: rawContent,
      contentBlocks: parsedResponse.content,
      usage: {
        inputTokens: data.usage?.input_tokens || 0,
        outputTokens: data.usage?.output_tokens || 0
      },
      cost: this.calculateCost(data.usage, profile.provider),
      duration: 0, // å°†åœ¨å¤–éƒ¨è®¾ç½®
      model: profile.modelName
    }
  }

  /**
   * å¤„ç† Anthropic SSE æµå¼å“åº”
   * äº‹ä»¶ç±»å‹å‚è§å®˜æ–¹ï¼šmessage_start/content_block_start/content_block_delta/.../message_delta/message_stop
   */
  private async handleAnthropicStreamingResponse(response: Response, profile: ModelProfile, request: AIRequest): Promise<AIResponse> {
    if (!response.body) throw new Error('Response body is null')

    const reader = response.body.getReader()
    const decoder = new TextDecoder('utf-8')
    let buffer = ''
    let content = ''

    const responseManager = getResponseStateManager()
    const streamId = `stream-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`
    responseManager.startStreaming(streamId)
    const isInteractiveUI = (global as any).WRITEFLOW_INTERACTIVE === true
    const useConsoleProgress = typeof request.onToken !== 'function' && !isInteractiveUI
    if (useConsoleProgress) {
      startStreamingProgress({ style: 'claude', showDuration: true, showTokens: true, showInterruptHint: true })
    }

    try {
      while (true) {
        const { value, done } = await reader.read()
        if (done) break
        buffer += decoder.decode(value, { stream: true })

        const parts = buffer.split('\n\n')
        buffer = parts.pop() || ''
        for (const part of parts) {
          const line = part.trim()
          if (!line.startsWith('data:')) continue
          const dataStr = line.slice(5).trim()
          if (!dataStr || dataStr === '[DONE]') continue
          try {
            const evt = JSON.parse(dataStr)
            // content_block_delta æºå¸¦æ–‡æœ¬å¢é‡
            if (evt.type === 'content_block_delta' && evt.delta?.type === 'text_delta' && evt.delta?.text) {
              const deltaText = evt.delta.text as string
              content += deltaText
              const estimated = Math.ceil(content.length / 4)
              responseManager.updateStreamingProgress(streamId, { tokenCount: estimated, characterCount: content.length, chunkSize: deltaText.length, contentType: 'text' })
              if (typeof request.onToken === 'function') {
                try { request.onToken(deltaText) } catch {}
              } else if (!isInteractiveUI) {
                process.stdout.write(deltaText)
              }
            }
          } catch {
            // å¿½ç•¥è§£æå¤±è´¥
          }
        }
      }
    } finally {
      reader.releaseLock()
      if (useConsoleProgress) stopStreamingProgress()
    }

    const finalTokens = Math.ceil(content.length / 4)
    const stats = responseManager.completeStreaming(streamId, finalTokens)
    const parsedResponse = parseAIResponse(content)

    return {
      content,
      contentBlocks: parsedResponse.content,
      usage: { inputTokens: 0, outputTokens: finalTokens },
      cost: 0,
      duration: stats.duration,
      model: profile.modelName,
    }
  }
  
  /**
   * è°ƒç”¨ DeepSeek API - æ”¯æŒåŸç”Ÿ function calling
   */
  private async callDeepSeekAPI(profile: ModelProfile, request: AIRequest): Promise<AIResponse> {
    const apiKey = this.getAPIKey(profile, ['DEEPSEEK_API_KEY'])
    if (!apiKey) {
      throw new Error('ç¼ºå°‘ DeepSeek API å¯†é’¥')
    }
    
    const url = profile.baseURL || 'https://api.deepseek.com/chat/completions'
    
    // å¦‚æœå¯ç”¨äº†å·¥å…·è°ƒç”¨ï¼Œåˆ™ä½¿ç”¨å¤šè½®å¯¹è¯æœºåˆ¶
    if (request.enableToolCalls && request.allowedTools && request.allowedTools.length > 0) {
      return await this.callDeepSeekWithTools(url, apiKey, profile, request)
    }
    
    // æ ‡å‡†è°ƒç”¨ï¼ˆæ— å·¥å…·ï¼‰
    const messages = []
    if (request.systemPrompt) {
      messages.push({ role: 'system', content: request.systemPrompt })
    }
    messages.push({ role: 'user', content: request.prompt })
    
    const payload = {
      model: profile.modelName,
      messages,
      max_tokens: request.maxTokens || profile.maxTokens,
      temperature: request.temperature || 0.3,
      stream: request.stream || false
    }
    
    const response = await fetch(url, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${apiKey}`
      },
      body: JSON.stringify(payload)
    })
    
    if (!response.ok) {
      const errorText = await response.text()
      throw new Error(`DeepSeek API é”™è¯¯: ${response.status} - ${errorText}`)
    }
    
    // å¦‚æœæ˜¯æµå¼è¯·æ±‚ï¼Œå¤„ç†æµå¼å“åº”
    if (request.stream) {
      return await this.handleStreamingResponse(response, profile, request)
    }
    
    // éæµå¼å¤„ç†
    const data = await response.json()
    
    const rawContent = data.choices?.[0]?.message?.content || 'æ— å“åº”å†…å®¹'
    const parsedResponse = parseAIResponse(rawContent)
    
    return {
      content: rawContent,
      contentBlocks: parsedResponse.content,
      usage: {
        inputTokens: data.usage?.prompt_tokens || 0,
        outputTokens: data.usage?.completion_tokens || 0
      },
      cost: this.calculateCost(data.usage, profile.provider),
      duration: 0,
      model: profile.modelName
    }
  }
  
  /**
   * å¤„ç†æµå¼å“åº”
   */
  private async handleStreamingResponse(response: Response, profile: ModelProfile, request: AIRequest): Promise<AIResponse> {
    if (!response.body) {
      throw new Error('Response body is null')
    }

    const reader = response.body.getReader()
    const decoder = new TextDecoder('utf-8')
    let buffer = ''
    let content = ''
    let usage = { inputTokens: 0, outputTokens: 0 }
    let pipeClosed = false
    
    // è·å–å“åº”çŠ¶æ€ç®¡ç†å™¨å¹¶å¼€å§‹æµå¼è·Ÿè¸ªï¼ˆåœ¨äº¤äº’å¼ UI ä¸‹ç¦ç”¨æ§åˆ¶å°è¾“å‡ºï¼‰
    const responseManager = getResponseStateManager()
    const streamId = `stream-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`
    responseManager.startStreaming(streamId)
    const isInteractiveUI = (global as any).WRITEFLOW_INTERACTIVE === true
    const useConsoleProgress = typeof request.onToken !== 'function' && !isInteractiveUI
    if (useConsoleProgress) {
      startStreamingProgress({ style: 'claude', showTokens: true, showDuration: true, showInterruptHint: true })
    }
    
    // ç›‘å¬ç®¡é“å…³é—­äº‹ä»¶
    process.stdout.on('error', (error) => {
      if ((error as any).code === 'EPIPE') {
        pipeClosed = true
      }
    })
    
    try {
      while (true) {
        const { done, value } = await reader.read()
        if (done) break

        buffer += decoder.decode(value, { stream: true })
        const parts = buffer.split('\n\n')
        buffer = parts.pop() || ''

        for (const part of parts) {
          const line = part.trim()
          if (!line.startsWith('data:')) continue
          const dataStr = line.slice(5).trim()
          if (dataStr === '[DONE]') continue
          try {
            const data = JSON.parse(dataStr)
            const delta = data.choices?.[0]?.delta?.content
            if (delta) {
              content += delta
              const estimatedTokens = Math.ceil(content.length / 4)
              responseManager.updateStreamingProgress(streamId, { tokenCount: estimatedTokens, characterCount: content.length, chunkSize: delta.length, contentType: 'text' })
              if (typeof request.onToken === 'function') {
                try { request.onToken(delta) } catch {}
              } else if (!isInteractiveUI && !process.stdout.destroyed && !pipeClosed) {
                try {
                  const canWrite = process.stdout.write(delta)
                  if (!canWrite) process.stdout.once('drain', () => {})
                } catch {
                  pipeClosed = true
                }
              }
            }
            if (data.usage) {
              usage.inputTokens = data.usage.prompt_tokens || 0
              usage.outputTokens = data.usage.completion_tokens || 0
            }
          } catch {
            // å¿½ç•¥è§£æå¤±è´¥
          }
        }
      }
    } finally {
      reader.releaseLock()
    }
    
    // å®Œæˆæµå¼å“åº”å¹¶è·å–ç»Ÿè®¡ä¿¡æ¯
    const finalTokenCount = usage.outputTokens || Math.ceil(content.length / 4)
    const streamingStats = responseManager.completeStreaming(streamId, finalTokenCount)
    const parsedResponse = parseAIResponse(content)
    
    // åœæ­¢è¿›åº¦æŒ‡ç¤ºå™¨ï¼ˆä»…æ§åˆ¶å°æ¨¡å¼ï¼‰
    if (useConsoleProgress) {
      stopStreamingProgress()
    }
    
    // åœ¨æµå¼å¤„ç†å®Œæˆåï¼Œæä¾›æ ¼å¼åŒ–åçš„æœ€ç»ˆè¾“å‡º
    if (useConsoleProgress) {
      try {
        const formatter = getOutputFormatter({
          enableColors: process.stdout.isTTY,
          theme: process.env.WRITEFLOW_THEME === 'light' ? 'light' : 'dark'
        })
        const formatted = formatter.formatStreamOutput(content, { maxWidth: 80 })
        if (formatted.hasCodeBlocks && formatted.codeBlockCount > 0) {
          process.stderr.write(`\n${formatter.formatSuccess(`åŒ…å« ${formatted.codeBlockCount} ä¸ªä»£ç å—çš„å†…å®¹å·²è¾“å‡º`)}\n`)
        }
      } catch (formatError) {
        console.warn(`æœ€ç»ˆæ ¼å¼åŒ–å¤±è´¥: ${formatError}`)
      }
    }
    
    return {
      content,
      contentBlocks: parsedResponse.content,
      usage,
      cost: this.calculateCost({
        prompt_tokens: usage.inputTokens,
        completion_tokens: usage.outputTokens
      }, profile.provider),
      duration: streamingStats.duration,
      model: profile.modelName,
      // æ·»åŠ æµå¼æ€§èƒ½ç»Ÿè®¡ï¼ˆå¯é€‰ï¼‰
      streamingStats: {
        duration: streamingStats.duration,
        tokenCount: finalTokenCount,
        tokensPerSecond: streamingStats.tokensPerSecond,
        startTime: streamingStats.startTime,
        endTime: streamingStats.endTime
      }
    }
  }

  /**
   * è°ƒç”¨ OpenAI API
   */
  private async callOpenAIAPI(profile: ModelProfile, request: AIRequest): Promise<AIResponse> {
    const apiKey = this.getAPIKey(profile, ['OPENAI_API_KEY'])
    if (!apiKey) {
      throw new Error('ç¼ºå°‘ OpenAI API å¯†é’¥')
    }
    
    const url = profile.baseURL || 'https://api.openai.com/v1/chat/completions'
    
    const messages = []
    if (request.systemPrompt) {
      messages.push({ role: 'system', content: request.systemPrompt })
    }
    messages.push({ role: 'user', content: request.prompt })
    
    const payload: any = {
      model: profile.modelName,
      messages,
      max_tokens: request.maxTokens || profile.maxTokens,
      temperature: request.temperature || 0.3,
    }
    if (request.stream) payload.stream = true
    
    const response = await fetch(url, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${apiKey}`
      },
      body: JSON.stringify(payload)
    })
    
    if (!response.ok) {
      const errorText = await response.text()
      throw new Error(`OpenAI API é”™è¯¯: ${response.status} - ${errorText}`)
    }
    if (request.stream) {
      return await this.handleStreamingResponse(response, profile, request)
    }

    const data = await response.json()
    
    const rawContent = data.choices?.[0]?.message?.content || 'æ— å“åº”å†…å®¹'
    const parsedResponse = parseAIResponse(rawContent)
    
    return {
      content: rawContent,
      contentBlocks: parsedResponse.content,
      usage: {
        inputTokens: data.usage?.prompt_tokens || 0,
        outputTokens: data.usage?.completion_tokens || 0
      },
      cost: this.calculateCost(data.usage, profile.provider),
      duration: 0,
      model: profile.modelName
    }
  }
  
  /**
   * è°ƒç”¨ Kimi API
   */
  private async callKimiAPI(profile: ModelProfile, request: AIRequest): Promise<AIResponse> {
    const apiKey = this.getAPIKey(profile, ['KIMI_API_KEY', 'MOONSHOT_API_KEY'])
    if (!apiKey) {
      throw new Error('ç¼ºå°‘ Kimi API å¯†é’¥')
    }
    
    const url = profile.baseURL || 'https://api.moonshot.cn/v1/chat/completions'
    
    const messages = []
    if (request.systemPrompt) {
      messages.push({ role: 'system', content: request.systemPrompt })
    }
    messages.push({ role: 'user', content: request.prompt })
    
    const payload: any = {
      model: profile.modelName,
      messages,
      max_tokens: request.maxTokens || profile.maxTokens,
      temperature: request.temperature || 0.3,
    }
    if (request.stream) payload.stream = true
    
    const response = await fetch(url, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${apiKey}`
      },
      body: JSON.stringify(payload)
    })
    
    if (!response.ok) {
      const errorText = await response.text()
      throw new Error(`Kimi API é”™è¯¯: ${response.status} - ${errorText}`)
    }
    if (request.stream) {
      return await this.handleStreamingResponse(response, profile, request)
    }

    const data = await response.json()
    
    const rawContent = data.choices?.[0]?.message?.content || 'æ— å“åº”å†…å®¹'
    const parsedResponse = parseAIResponse(rawContent)
    
    return {
      content: rawContent,
      contentBlocks: parsedResponse.content,
      usage: {
        inputTokens: data.usage?.prompt_tokens || 0,
        outputTokens: data.usage?.completion_tokens || 0
      },
      cost: this.calculateCost(data.usage, profile.provider),
      duration: 0,
      model: profile.modelName
    }
  }
  
  /**
   * è·å–é»˜è®¤æ¨¡å‹åç§°
   */
  private getDefaultModelName(): string {
    // ä¼˜å…ˆä½¿ç”¨ AI_MODEL ç¯å¢ƒå˜é‡
    if (process.env.AI_MODEL) {
      return process.env.AI_MODEL
    }
    
    // å…¶æ¬¡ä½¿ç”¨ API_PROVIDER æ¨æ–­æ¨¡å‹
    const provider = process.env.API_PROVIDER
    switch (provider) {
      case 'deepseek':
        return 'deepseek-chat'
      case 'qwen3':
        return 'qwen-turbo'
      case 'glm4.5':
        return 'glm-4-flash'
      case 'anthropic':
        return 'claude-3-sonnet-20240229'
      case 'openai':
        return 'gpt-3.5-turbo'
      case 'kimi':
        return 'moonshot-v1-8k'
      default:
        // æœ€åæ£€æŸ¥æœ‰å“ªäº› API Key å¯ç”¨ï¼Œæ™ºèƒ½é€‰æ‹©é»˜è®¤æ¨¡å‹
        if (process.env.DEEPSEEK_API_KEY) return 'deepseek-chat'
        if (process.env.ANTHROPIC_API_KEY) return 'claude-3-sonnet-20240229'
        if (process.env.OPENAI_API_KEY) return 'gpt-3.5-turbo'
        if (process.env.KIMI_API_KEY) return 'moonshot-v1-8k'
        if (process.env.GLM_API_KEY) return 'glm-4-flash'
        
        return 'deepseek-chat' // æœ€ç»ˆé»˜è®¤ä½¿ç”¨ DeepSeek
    }
  }

  /**
   * æ ¹æ®æ¨¡å‹åç§°åˆ›å»ºä¸´æ—¶çš„æ¨¡å‹é…ç½®
   */
  private createTempModelProfile(modelName: string): ModelProfile | null {
    // æ ¹æ®æ¨¡å‹åç§°æ¨æ–­æä¾›å•†
    let provider: string
    let baseURL: string | undefined
    
    if (modelName.includes('deepseek')) {
      provider = 'deepseek'
      baseURL = 'https://api.deepseek.com/v1/chat/completions'
    } else if (modelName.includes('claude') || modelName.includes('anthropic')) {
      provider = 'anthropic'
      baseURL = 'https://api.anthropic.com/v1/messages'
    } else if (modelName.includes('gpt') || modelName.includes('openai')) {
      provider = 'openai'
      baseURL = 'https://api.openai.com/v1/chat/completions'
    } else if (modelName.includes('moonshot') || modelName.includes('kimi')) {
      provider = 'kimi'
      baseURL = 'https://api.moonshot.cn/v1/chat/completions'
    } else if (modelName.includes('qwen')) {
      provider = 'openai' // Qwen ä½¿ç”¨ OpenAI å…¼å®¹åè®®
      baseURL = process.env.API_BASE_URL || 'https://dashscope.aliyuncs.com/compatible-mode/v1'
    } else if (modelName.includes('glm')) {
      provider = 'openai' // GLM ä½¿ç”¨ OpenAI å…¼å®¹åè®®
      baseURL = process.env.API_BASE_URL || 'https://open.bigmodel.cn/api/paas/v4'
    } else {
      // æ ¹æ®ç¯å¢ƒå˜é‡æ¨æ–­
      provider = process.env.API_PROVIDER || 'deepseek'
      baseURL = process.env.API_BASE_URL
    }

    // åˆ›å»ºä¸´æ—¶çš„æ¨¡å‹é…ç½®
    const profile: ModelProfile = {
      name: `temp-${modelName}`,
      provider: provider as any,
      modelName: modelName,
      baseURL: baseURL,
      apiKey: this.getAPIKeyForProvider(provider),
      maxTokens: 4000,
      contextLength: 128000,
      isActive: true
    }

    // éªŒè¯ API å¯†é’¥æ˜¯å¦å¯ç”¨
    if (!profile.apiKey) {
      console.warn(`æ‰¾ä¸åˆ° ${provider} çš„ API å¯†é’¥`)
      return null
    }

    return profile
  }

  /**
   * æ ¹æ®æä¾›å•†è·å– API å¯†é’¥
   */
  private getAPIKeyForProvider(provider: string): string {
    const envKeys = {
      anthropic: ['ANTHROPIC_API_KEY', 'CLAUDE_API_KEY'],
      deepseek: ['DEEPSEEK_API_KEY'],
      openai: ['OPENAI_API_KEY'],
      kimi: ['KIMI_API_KEY', 'MOONSHOT_API_KEY'],
      qwen: ['QWEN_API_KEY', 'DASHSCOPE_API_KEY'],
      glm: ['GLM_API_KEY', 'ZHIPUAI_API_KEY']
    }

    const keys = envKeys[provider as keyof typeof envKeys] || []
    for (const key of keys) {
      const value = process.env[key]
      if (value) return value
    }

    return ''
  }

  /**
   * è·å– API å¯†é’¥
   */
  private getAPIKey(profile: ModelProfile, envKeys: string[]): string | undefined {
    // ä¼˜å…ˆä½¿ç”¨é…ç½®ä¸­çš„å¯†é’¥
    if (profile.apiKey) {
      return profile.apiKey
    }
    
    // ä»ç¯å¢ƒå˜é‡è·å–
    for (const key of envKeys) {
      const value = process.env[key]
      if (value) {
        return value
      }
    }
    
    return undefined
  }
  
  /**
   * æŸ¥æ‰¾æ¨¡å‹é…ç½®
   */
  private findModelProfile(modelName: string): ModelProfile | null {
    const profiles = this.modelManager.getAllProfiles()
    return profiles.find(p => p.modelName === modelName || p.name === modelName) || null
  }
  
  /**
   * è®¡ç®—æˆæœ¬
   */
  private calculateCost(usage: any, provider: string): number {
    if (!usage) return 0
    
    // ç®€åŒ–çš„æˆæœ¬è®¡ç®—
    const inputTokens = usage.prompt_tokens || usage.input_tokens || 0
    const outputTokens = usage.completion_tokens || usage.output_tokens || 0
    
    // åŸºç¡€è´¹ç‡ï¼ˆå®é™…è´¹ç‡åº”è¯¥ä»æ¨¡å‹é…ç½®ä¸­è·å–ï¼‰
    const rates = {
      anthropic: { input: 0.000003, output: 0.000015 },
      deepseek: { input: 0.00000027, output: 0.0000011 },
      openai: { input: 0.0000025, output: 0.00001 },
      kimi: { input: 0.000001, output: 0.000002 },
      bigdream: { input: 0.000003, output: 0.000015 }
    }
    
    const rate = rates[provider as keyof typeof rates] || { input: 0, output: 0 }
    return inputTokens * rate.input + outputTokens * rate.output
  }

  /**
   * è°ƒç”¨ DeepSeek API çš„å·¥å…·è°ƒç”¨ç‰ˆæœ¬ - å¤šè½®å¯¹è¯
   */
  private async callDeepSeekWithTools(url: string, apiKey: string, profile: ModelProfile, request: AIRequest): Promise<AIResponse> {
    const messages = []
    if (request.systemPrompt) {
      messages.push({ role: 'system', content: request.systemPrompt })
    }
    messages.push({ role: 'user', content: request.prompt })

    // è½¬æ¢å·¥å…·å®šä¹‰
    const tools = await this.convertToolsToDeepSeekFormat(request.allowedTools!)
    
    // å¦‚æœæ²¡æœ‰å·¥å…·æˆ–å·¥å…·ä¸ºç©ºï¼Œå›é€€åˆ°æ ‡å‡†è°ƒç”¨
    if (!tools || tools.length === 0) {
      return await this.callDeepSeekAPI(profile, { ...request, enableToolCalls: false, allowedTools: undefined })
    }
    
    let totalInputTokens = 0
    let totalOutputTokens = 0
    let conversationHistory = ''
    let maxIterations = 5
    let iteration = 0
    let consecutiveFailures = 0
    const maxConsecutiveFailures = 2
    let lastRoundHadTodoUpdate = false

    while (iteration < maxIterations) {
      console.log(`ğŸ”„ AI æ­£åœ¨æ€è€ƒå’Œæ‰§è¡Œ...`)
      
      const payload: any = {
        model: profile.modelName,
        messages,
        tools: lastRoundHadTodoUpdate ? [] : tools,
        tool_choice: lastRoundHadTodoUpdate ? 'none' : 'auto',
        max_tokens: request.maxTokens || profile.maxTokens,
        temperature: request.temperature || 0.3,
        // æ³¨æ„ï¼šå¸¦å·¥å…·è°ƒç”¨çš„æµå¼å“åº”æ˜¯ SSEï¼ŒåŒ…å« `data:` å‰ç¼€ï¼Œ
        // è¿™é‡Œç»Ÿä¸€å…³é—­æµå¼ï¼Œæ”¹ä¸ºä¸€æ¬¡æ€§ JSONï¼Œé¿å…è§£ææŠ¥é”™ã€‚
        stream: false
      }

      const response: any = await fetch(url, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${apiKey}`
        },
        body: JSON.stringify(payload)
      })

      if (!response.ok) {
        const errorText = await response.text()
        throw new Error(`DeepSeek API é”™è¯¯: ${response.status} - ${errorText}`)
      }

      let data: any
      try {
        data = await response.json()
      } catch (e) {
        // æŸäº›ç½‘å…³å¯èƒ½ä»è¿”å› SSEï¼Œè¿™é‡Œå…œåº•è¯»å–æ–‡æœ¬å¹¶å°è¯•æå–æœ€åä¸€ä¸ª data: JSON
        const text = await response.text()
        const lines = text.split(/\n/).map((l: string) => l.trim()).filter(Boolean)
        const lastData = [...lines].reverse().find((l: string) => l.startsWith('data:'))
        if (!lastData) throw e
        const jsonStr = lastData.replace(/^data:\s*/, '')
        data = JSON.parse(jsonStr)
      }
      const message: any = data.choices?.[0]?.message
      // å¤„ç† DeepSeek å†…è”å·¥å…·æ ‡è®°ï¼ˆè‹¥å­˜åœ¨ï¼‰
      if (message && typeof message.content === 'string' && message.content.includes('toolâ–')) {
        const inline = this.extractInlineToolCalls(message.content)
        message.content = inline.cleaned
        if (inline.calls.length > 0) {
          message.tool_calls = (message.tool_calls || []).concat(
            inline.calls.map((c: any) => ({
              type: 'function',
              id: `inline_${Date.now()}_${Math.random().toString(36).slice(2)}`,
              function: {
                name: c.name,
                arguments: JSON.stringify(c.args)
              }
            }))
          )
        }
      }
      
      totalInputTokens += data.usage?.prompt_tokens || 0
      totalOutputTokens += data.usage?.completion_tokens || 0

      // å¦‚æœAIæ²¡æœ‰è°ƒç”¨å·¥å…·ï¼Œåˆ™å¯¹è¯ç»“æŸ
      if (!message.tool_calls || message.tool_calls.length === 0) {
        conversationHistory += this.sanitizeLLMArtifacts(message.content)

        // è‹¥ä¸Šä¸€è½®åˆšè¿›è¡Œäº† todo_* æ›´æ–°ï¼Œè¿™ä¸€è½®æ˜¯æ­£æ–‡ç”Ÿæˆï¼š
        // 1) è‡ªåŠ¨å°†å½“å‰ in_progress ç½®ä¸º completed
        // 2) è‹¥è¾“å‡ºæ–‡æœ¬è¶³å¤Ÿå®Œæ•´ï¼ˆé•¿åº¦é˜ˆå€¼ï¼‰ï¼Œå°†å‰©ä½™ pending ä¹Ÿæ ‡è®°ä¸º completedï¼ˆé¿å…ä¸€æ¬¡æ€§å®Œæˆçš„åœºæ™¯è¿›åº¦ä¸åŒæ­¥ï¼‰
        if (lastRoundHadTodoUpdate) {
          try {
            const { TodoManager } = await import('../../tools/TodoManager.js')
            const mgr = new TodoManager(process.env.WRITEFLOW_SESSION_ID)
            const current = await mgr.getCurrentTask()
            let changed = false
            if (current) {
              await mgr.completeTask(current.id)
              changed = true
            }
            // å¦‚æœæ–‡æœ¬è¾ƒé•¿ï¼Œè®¤ä¸ºæœ¬è½®å®Œæˆäº†ä¸»è¦å·¥ä½œï¼Œæ‰¹é‡åŒæ­¥å‰©ä½™å¾…å¤„ç†ä¸ºå®Œæˆ
            const substantial = (message.content || '').length >= 800 || conversationHistory.length >= 1200
            if (substantial) {
              const all = await mgr.getAllTodos()
              const pendingIds = all.filter(t => t.status === 'pending').map(t => t.id)
              if (pendingIds.length > 0) {
                await mgr.batchUpdateStatus(pendingIds, 'completed' as any)
                changed = true
              }
            }
            if (changed) emitReminderEvent('todo:changed', { agentId: 'deepseek-ai' })
          } catch (e) {
            console.warn('âš ï¸ è‡ªåŠ¨å®Œæˆå½“å‰ä»»åŠ¡å¤±è´¥:', (e as Error)?.message)
          }
        }
        
        return {
          content: conversationHistory,
          usage: {
            inputTokens: totalInputTokens,
            outputTokens: totalOutputTokens
          },
          cost: this.calculateCost({ prompt_tokens: totalInputTokens, completion_tokens: totalOutputTokens }, profile.provider),
          duration: 0,
          model: profile.modelName,
          hasToolInteraction: iteration > 0
        }
      }

      // AI è°ƒç”¨äº†å·¥å…·ï¼Œæ·»åŠ  AI æ¶ˆæ¯åˆ°å¯¹è¯å†å²
      messages.push(message)
      
      // æ‰§è¡Œå·¥å…·è°ƒç”¨
      let currentRoundHasFailures = false
      let currentRoundHasTodoUpdate = false
      for (const toolCall of message.tool_calls) {
        console.log(`ğŸ”§ [${toolCall.function.name}] æ­£åœ¨æ‰§è¡Œ...`)
        // è¿‡æ»¤TODOå·¥å…·çš„æ‰§è¡Œä¿¡æ¯ï¼Œä¸æ·»åŠ åˆ°conversation historyä¸­
        if (!toolCall.function.name.includes('todo')) {
          conversationHistory += `\nAI: [è°ƒç”¨ ${toolCall.function.name} å·¥å…·] æ­£åœ¨æ‰§è¡Œ...\n`
        }
        
        try {
          const toolResult = await this.executeDeepSeekToolCall(toolCall)
          
          if (toolResult.success) {
            console.log(`âœ… [${toolCall.function.name}] ${toolResult.result}`)
            // è¿‡æ»¤TODOå·¥å…·ç»“æœï¼Œä¸æ·»åŠ åˆ°conversation historyä¸­
            if (!toolCall.function.name.includes('todo')) {
              conversationHistory += `${toolCall.function.name}å·¥å…·: ${toolResult.result}\n`
            }
            consecutiveFailures = 0 // é‡ç½®è¿ç»­å¤±è´¥è®¡æ•°
            if (toolCall.function.name.startsWith('todo_')) {
              currentRoundHasTodoUpdate = true
            }
          } else {
            console.log(`âŒ [${toolCall.function.name}] ${toolResult.error}`)
            // TODOå·¥å…·çš„é”™è¯¯ä¹Ÿä¸æ·»åŠ åˆ°conversation historyä¸­
            if (!toolCall.function.name.includes('todo')) {
              conversationHistory += `${toolCall.function.name}å·¥å…·: ${toolResult.error}\n`
            }
            currentRoundHasFailures = true
          }
          
          // å°†å·¥å…·æ‰§è¡Œç»“æœæ·»åŠ åˆ°æ¶ˆæ¯å†å²
          messages.push({
            role: 'tool',
            tool_call_id: toolCall.id,
            content: toolResult.success ? toolResult.result : toolResult.error || 'æ‰§è¡Œå¤±è´¥'
          })
        } catch (error) {
          const errorMsg = `å·¥å…·æ‰§è¡Œå¤±è´¥: ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}`
          console.log(`âŒ [${toolCall.function.name}] ${errorMsg}`)
          conversationHistory += `${toolCall.function.name}å·¥å…·: ${errorMsg}\n`
          currentRoundHasFailures = true
          
          messages.push({
            role: 'tool',
            tool_call_id: toolCall.id,
            content: errorMsg
          })
        }
      }

      // æ£€æŸ¥è¿ç»­å¤±è´¥
      if (currentRoundHasFailures) {
        consecutiveFailures++
        if (consecutiveFailures >= maxConsecutiveFailures) {
          console.log(`âš ï¸  è¿ç»­å¤±è´¥ ${consecutiveFailures} æ¬¡ï¼Œç»ˆæ­¢å·¥å…·è°ƒç”¨`)
          return {
            content: conversationHistory + '\nç³»ç»Ÿæç¤ºï¼šè¿ç»­å·¥å…·è°ƒç”¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥å‚æ•°æ ¼å¼å’Œå·¥å…·ä½¿ç”¨æ–¹æ³•ã€‚',
            usage: {
              inputTokens: totalInputTokens,
              outputTokens: totalOutputTokens
            },
            cost: this.calculateCost({ prompt_tokens: totalInputTokens, completion_tokens: totalOutputTokens }, profile.provider),
            duration: 0,
            model: profile.modelName,
            hasToolInteraction: true
          }
        }
      }
      // è‹¥æœ¬è½®æˆåŠŸæ›´æ–°äº† todo_*ï¼Œä¸‹ä¸€è½®ç¦ç”¨å·¥å…·å¹¶å¼ºåˆ¶æ­£æ–‡ç”Ÿæˆ
      if (currentRoundHasTodoUpdate) {
        lastRoundHadTodoUpdate = true
        messages.push({
          role: 'user',
          content: 'å·²æ›´æ–°ä»»åŠ¡åˆ—è¡¨ã€‚ç°åœ¨è¯·æ ¹æ®å½“å‰ä»»åŠ¡ç›´æ¥ç”Ÿæˆæ­£æ–‡å†…å®¹ï¼Œä¸è¦å†è°ƒç”¨ä»»ä½•å·¥å…·ã€‚è¯·å¼€å§‹å†™ä½œã€‚'
        })
      } else {
        lastRoundHadTodoUpdate = false
      }

      iteration++
    }

    // è¶…è¿‡æœ€å¤§è¿­ä»£æ¬¡æ•°
    return {
      content: conversationHistory + '\n[ç³»ç»Ÿ] å¯¹è¯è¾¾åˆ°è½®æ¬¡ä¸Šé™ã€‚è¯·ç»§ç»­ç›´æ¥ç”Ÿæˆæ­£æ–‡å†…å®¹ã€‚',
      usage: {
        inputTokens: totalInputTokens,
        outputTokens: totalOutputTokens
      },
      cost: this.calculateCost({ prompt_tokens: totalInputTokens, completion_tokens: totalOutputTokens }, profile.provider),
      duration: 0,
      model: profile.modelName,
      hasToolInteraction: true
    }
  }

  /**
   * è½¬æ¢å·¥å…·å®šä¹‰ä¸º DeepSeek API æ ¼å¼ - ä½¿ç”¨æ–°çš„å·¥å…·ç³»ç»Ÿ
   * ä¼˜å…ˆè€ƒè™‘æƒé™å’Œå¯ç”¨æ€§æ£€æŸ¥
   */
  private async convertToolsToDeepSeekFormat(allowedTools: string[]): Promise<any[]> {
    const tools = []
    
    // å¦‚æœæ²¡æœ‰å…è®¸çš„å·¥å…·ï¼Œç›´æ¥è¿”å›ç©ºæ•°ç»„
    if (!allowedTools || allowedTools.length === 0) {
      return []
    }
    
    // è·å–å½“å‰å¯ç”¨çš„å·¥å…·ï¼ˆè€ƒè™‘æƒé™ï¼‰
    const availableTools = this.toolOrchestrator.getAvailableTools()
    const availableToolNames = new Set(availableTools.map(t => t.name))
    
    for (const toolName of allowedTools) {
      // æ£€æŸ¥å·¥å…·æ˜¯å¦åœ¨å…è®¸çš„å·¥å…·åˆ—è¡¨ä¸­
      // å†…ç½®å…¼å®¹: ä¸€äº›å†™ä½œåŸŸå·¥å…·ï¼ˆå¦‚ todo_*ã€exit_plan_modeï¼‰æœªæ¥å…¥ç¼–æ’å™¨
      // è¿™é‡Œç›´æ¥æä¾›æœ€å° JSON-Schema æè¿°ï¼Œé¿å…æ§åˆ¶å°å‡ºç°å™ªéŸ³æ—¥å¿—å¹¶å…è®¸æ¨¡å‹åŸç”Ÿå‡½æ•°è°ƒç”¨ã€‚
      if (!availableToolNames.has(toolName)) {
        if (toolName === 'todo_write') {
          tools.push({
            type: 'function',
            function: {
              name: 'todo_write',
              description: 'ä»…ç”¨äºè¿›åº¦è¿½è¸ªçš„åå°å·¥å…·ï¼Œæ›´æ–°ä»»åŠ¡çŠ¶æ€ã€‚é‡è¦ï¼šè°ƒç”¨æ­¤å·¥å…·åå¿…é¡»ç»§ç»­æ‰§è¡Œç”¨æˆ·è¯·æ±‚çš„ä¸»è¦ä»»åŠ¡ï¼ˆå¦‚å†™æ•…äº‹ã€æ–‡ç« ç­‰ï¼‰ã€‚æ­¤å·¥å…·ä¸æ›¿ä»£å®é™…å†…å®¹ç”Ÿæˆã€‚',
              parameters: {
                type: 'object',
                properties: {
                  todos: {
                    type: 'array',
                    items: {
                      type: 'object',
                      properties: {
                        id: { type: 'string' },
                        content: { type: 'string' },
                        activeForm: { type: 'string' },
                        status: { type: 'string', enum: ['pending','in_progress','completed'] },
                        priority: { type: 'string', enum: ['high','medium','low'] }
                      },
                      required: ['id','content','activeForm','status']
                    }
                  }
                },
                required: ['todos']
              }
            }
          })
          continue
        }
        if (toolName === 'todo_read') {
          tools.push({
            type: 'function',
            function: {
              name: 'todo_read',
              description: 'è¯»å–å½“å‰ä»»åŠ¡åˆ—è¡¨å¹¶è¿”å› JSONã€‚',
              parameters: { type: 'object', properties: {}, additionalProperties: false }
            }
          })
          continue
        }
        if (toolName === 'exit_plan_mode') {
          tools.push({
            type: 'function',
            function: {
              name: 'exit_plan_mode',
              description: 'é€€å‡ºè®¡åˆ’æ¨¡å¼ï¼Œæ¢å¤æ­£å¸¸å¯¹è¯ã€‚',
              parameters: { type: 'object', properties: { plan: { type: 'string' } }, required: [] }
            }
          })
          continue
        }
        console.warn(`å·¥å…· ${toolName} ä¸åœ¨å¯ç”¨å·¥å…·åˆ—è¡¨ä¸­ï¼Œè·³è¿‡`)
        continue
      }
      
      const tool = this.toolOrchestrator.getTool(toolName)
      if (!tool) continue

      try {
        // è·å–å·¥å…·çš„å®Œæ•´æè¿°
        const description = await tool.prompt?.({ safeMode: false }) || await tool.description()
        
        // ç”Ÿæˆ JSON schema - ä½¿ç”¨å·¥å…·çš„å†…ç½®æ–¹æ³•
        let parameters: any
        if (tool.inputJSONSchema) {
          parameters = tool.inputJSONSchema
        } else {
          // å›é€€åˆ°ä¼ ç»Ÿè½¬æ¢æ–¹æ³•
          parameters = this.zodSchemaToJsonSchema(tool.inputSchema)
        }

        tools.push({
          type: 'function',
          function: {
            name: tool.name,
            description: `${description}\n\næƒé™çº§åˆ«: ${tool.isReadOnly() ? 'åªè¯»' : 'å¯å†™'}\nå¹¶å‘å®‰å…¨: ${tool.isConcurrencySafe() ? 'æ˜¯' : 'å¦'}`,
            parameters
          }
        })
        
        console.log(`âœ… å·¥å…· ${toolName} å·²æ·»åŠ åˆ° API è°ƒç”¨ä¸­`)
      } catch (error) {
        console.warn(`è½¬æ¢å·¥å…· ${toolName} åˆ° DeepSeek æ ¼å¼å¤±è´¥:`, error)
        
        // ä½¿ç”¨åŸºç¡€æè¿°ä½œä¸ºåå¤‡
        try {
          const basicDescription = await tool.description()
          tools.push({
            type: 'function',
            function: {
              name: tool.name,
              description: basicDescription,
              parameters: {
                type: 'object',
                properties: {},
                required: [],
                additionalProperties: true
              }
            }
          })
          console.log(`âš ï¸  å·¥å…· ${toolName} ä½¿ç”¨åŸºç¡€æ ¼å¼æ·»åŠ `)
        } catch (fallbackError) {
          console.error(`å·¥å…· ${toolName} å®Œå…¨è½¬æ¢å¤±è´¥:`, fallbackError)
        }
      }
    }

    if (tools.length > 0) {
      console.log(`ğŸ”§ å…±è½¬æ¢ ${tools.length} ä¸ªå·¥å…·ä¾› AI ä½¿ç”¨`)
    }

    return tools
  }

  /**
   * å°† Zod schema è½¬æ¢ä¸º JSON Schema
   */
  private zodSchemaToJsonSchema(zodSchema: any): any {
    // ç®€åŒ–çš„ Zod åˆ° JSON Schema è½¬æ¢
    // åœ¨å®é™…é¡¹ç›®ä¸­ï¼Œå»ºè®®ä½¿ç”¨ zod-to-json-schema åº“
    const shape = zodSchema._def?.shape
    if (!shape) {
      return {
        type: 'object',
        properties: {},
        required: [],
        additionalProperties: false
      }
    }

    const properties: any = {}
    const required: string[] = []

    for (const [key, zodType] of Object.entries(shape)) {
      const fieldSchema = this.zodTypeToJsonSchema(zodType as any)
      properties[key] = fieldSchema
      
      // æ£€æŸ¥æ˜¯å¦æ˜¯å¿…éœ€å­—æ®µ
      if (!(zodType as any)._def?.optional) {
        required.push(key)
      }
    }

    return {
      type: 'object',
      properties,
      required,
      additionalProperties: false
    }
  }

  /**
   * å°† Zod ç±»å‹è½¬æ¢ä¸º JSON Schema å­—æ®µ
   */
  private zodTypeToJsonSchema(zodType: any): any {
    const typeName = zodType._def.typeName
    
    switch (typeName) {
      case 'ZodString':
        return {
          type: 'string',
          description: zodType.description || ''
        }
      case 'ZodNumber':
        return {
          type: 'number', 
          description: zodType.description || ''
        }
      case 'ZodBoolean':
        return {
          type: 'boolean',
          description: zodType.description || ''
        }
      case 'ZodOptional':
        return this.zodTypeToJsonSchema(zodType._def.innerType)
      case 'ZodDefault':
        const innerSchema = this.zodTypeToJsonSchema(zodType._def.innerType)
        innerSchema.default = zodType._def.defaultValue()
        return innerSchema
      case 'ZodArray':
        return {
          type: 'array',
          items: this.zodTypeToJsonSchema(zodType._def.type),
          description: zodType.description || ''
        }
      default:
        return {
          type: 'string',
          description: zodType.description || ''
        }
    }
  }

  /**
   * å®‰å…¨çš„ JSON è§£æï¼Œå¤„ç†è½¬ä¹‰å­—ç¬¦é—®é¢˜
   */
  private safeJSONParse(jsonStr: string): any {
    try {
      return JSON.parse(jsonStr)
    } catch (error) {
      console.log(`ğŸ”§ å°è¯•ä¿®å¤ JSON æ ¼å¼...`)
      
      // å°è¯•ä¿®å¤å¸¸è§çš„ JSON è½¬ä¹‰é—®é¢˜
      let fixedJson = jsonStr
        // ä¿®å¤æ¢è¡Œç¬¦
        .replace(/\\n/g, '\\\\n')
        // ä¿®å¤åˆ¶è¡¨ç¬¦
        .replace(/\\t/g, '\\\\t')
        // ä¿®å¤å›è½¦ç¬¦
        .replace(/\\r/g, '\\\\r')
        // ä¿®å¤åæ–œæ 
        .replace(/\\\\/g, '\\\\\\\\')
        // ä¿®å¤å•ç‹¬çš„å¼•å·è½¬ä¹‰
        .replace(/\\"/g, '\\\\"')
      
      try {
        return JSON.parse(fixedJson)
      } catch (retryError) {
        // æœ€åå°è¯•ï¼šç§»é™¤é—®é¢˜å­—ç¬¦å¹¶é‡æ–°è§£æ
        try {
          const cleanJson = jsonStr
            .replace(/[\x00-\x1F\x7F]/g, '') // ç§»é™¤æ§åˆ¶å­—ç¬¦
            .replace(/\\(?!["\\/bfnrtu])/g, '\\\\') // ä¿®å¤æ— æ•ˆè½¬ä¹‰
          
          return JSON.parse(cleanJson)
        } catch (finalError) {
          throw new Error(`JSON è§£æå¤±è´¥ - åŸå§‹é”™è¯¯: ${error instanceof Error ? error.message : 'æœªçŸ¥'}, é‡è¯•é”™è¯¯: ${retryError instanceof Error ? retryError.message : 'æœªçŸ¥'}, æœ€ç»ˆé”™è¯¯: ${finalError instanceof Error ? finalError.message : 'æœªçŸ¥'}`)
        }
      }
    }
  }

  /**
   * æ‰§è¡Œ DeepSeek API çš„å·¥å…·è°ƒç”¨ - ä½¿ç”¨æ–°çš„å·¥å…·ç¼–æ’å™¨
   */
  private async executeDeepSeekToolCall(toolCall: any): Promise<AIToolExecutionResult> {
    const { name: toolName, arguments: argsStr } = toolCall.function
    
    // å®‰å…¨çš„ JSON è§£æ
    let args: any
    try {
      args = this.safeJSONParse(argsStr)
      console.log(`ğŸ”§ [${toolName}] è§£æå‚æ•°:`, JSON.stringify(args, null, 2))
    } catch (parseError) {
      console.error(`âŒ [${toolName}] JSON è§£æå¤±è´¥ï¼ŒåŸå§‹å­—ç¬¦ä¸²:`, argsStr)
      return {
        toolName,
        callId: toolCall.id,
        result: '',
        success: false,
        error: `å·¥å…·è°ƒç”¨å‚æ•° JSON è§£æå¤±è´¥: ${parseError instanceof Error ? parseError.message : 'æœªçŸ¥é”™è¯¯'}`
      }
    }

    try {
      // åˆ›å»ºå·¥å…·æ‰§è¡Œä¸Šä¸‹æ–‡
      const toolContext: ToolUseContext = {
        messageId: `deepseek-${toolCall.id}`,
        agentId: 'deepseek-ai',
        safeMode: false,
        abortController: new AbortController(),
        readFileTimestamps: {},
        options: {
          verbose: true,
          safeMode: false,
          messageLogName: 'deepseek-ai'
        }
      }

      // è‹¥ç¼–æ’å™¨æ²¡æœ‰æ³¨å†Œè¯¥å·¥å…·ï¼Œç›´æ¥èµ°æ—§åŸŸå·¥å…·æ‰§è¡Œ
      if (!this.toolOrchestrator.getTool(toolName)) {
        const legacy = await this.executeLegacyTool(toolName, args)
        if (legacy) {
          return {
            toolName,
            callId: toolCall.id,
            result: legacy.result,
            success: legacy.success,
            error: legacy.error
          }
        }
      }

      // ä½¿ç”¨å·¥å…·ç¼–æ’å™¨æ‰§è¡Œå·¥å…·è°ƒç”¨
      const executionResult = await this.toolOrchestrator.executeTool({
        toolName,
        input: args,
        context: toolContext,
        priority: 5 // ä¸­ç­‰ä¼˜å…ˆçº§
      })

      // è½¬æ¢ä¸ºæ—§æ ¼å¼çš„ç»“æœï¼ˆå…¼å®¹æ€§ï¼‰
      if (executionResult.status === ToolExecutionStatus.COMPLETED) {
        return {
          toolName,
          callId: toolCall.id,
          result: this.formatToolResult(executionResult.result, toolName),
          success: true
        }
      } else {
        // å¦‚æœæ˜¯æœªæ‰¾åˆ°ä¹‹ç±»çš„é”™è¯¯ï¼Œé€€å›æ—§åŸŸå·¥å…·æ‰§è¡Œ
        if (executionResult.error?.message?.includes('æœªæ‰¾åˆ°') || executionResult.error?.message?.includes('not found')) {
          const legacy = await this.executeLegacyTool(toolName, args)
          if (legacy) {
            return {
              toolName,
              callId: toolCall.id,
              result: legacy.result,
              success: legacy.success,
              error: legacy.error
            }
          }
        }
        return {
          toolName,
          callId: toolCall.id,
          result: '',
          success: false,
          error: executionResult.error?.message || 'å·¥å…·æ‰§è¡Œå¤±è´¥'
        }
      }
    } catch (error) {
      return {
        toolName,
        callId: toolCall.id,
        result: '',
        success: false,
        error: error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'
      }
    }
  }

  /**
   * æ ¼å¼åŒ–å·¥å…·æ‰§è¡Œç»“æœ
   */
  private formatToolResult(result: any, toolName: string): string {
    if (typeof result === 'string') {
      return result
    }
    
    if (toolName === 'Write' && result?.success) {
      return `æ–‡ä»¶å·²æˆåŠŸå†™å…¥ ${result.filePath} (${result.bytesWritten} å­—èŠ‚)`
    }
    
    if (toolName === 'Read' && result?.content) {
      return `æ–‡ä»¶å†…å®¹å·²è¯»å–ï¼Œé•¿åº¦: ${result.content.length} å­—ç¬¦`
    }
    
    if (toolName === 'Bash' && typeof result === 'object') {
      return result.output || result.stdout || JSON.stringify(result)
    }
    
    return JSON.stringify(result)
  }

  /**
   * å¤„ç†å·¥å…·äº¤äº’ (æ—§ç‰ˆæœ¬ï¼Œå·²è¢« DeepSeek åŸç”Ÿæ”¯æŒæ›¿ä»£)
   */
  private async processToolInteractions(response: AIResponse, request: AIRequest): Promise<AIResponse> {
    let currentContent = response.content
    let iterationCount = 0
    const maxIterations = 3 // é˜²æ­¢æ— é™å¾ªç¯

    while (iterationCount < maxIterations) {
      // æ£€æµ‹å·¥å…·è°ƒç”¨
      const toolCalls = this.detectToolCalls(currentContent, request.allowedTools!)
      
      if (toolCalls.length === 0) {
        // æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç»“æŸå¤„ç†
        break
      }

      console.log(`ğŸ”§ æ£€æµ‹åˆ° ${toolCalls.length} ä¸ªå·¥å…·è°ƒç”¨`)
      
      // æ‰§è¡Œå·¥å…·è°ƒç”¨
      const toolResults: AIToolExecutionResult[] = []
      for (const toolCall of toolCalls) {
        const result = await this.executeToolCall(toolCall)
        toolResults.push(result)
        
        // æ˜¾ç¤ºå·¥å…·æ‰§è¡Œè¿‡ç¨‹
        if (result.success) {
          console.log(`âœ… [${result.toolName}] ${result.result}`)
        } else {
          console.log(`âŒ [${result.toolName}] ${result.error}`)
        }
      }

      // å°†å·¥å…·ç»“æœé›†æˆåˆ°å“åº”ä¸­
      currentContent = this.integrateToolResults(currentContent, toolResults)
      response.hasToolInteraction = true
      
      iterationCount++
    }

    response.content = currentContent
    return response
  }

  /**
   * æ£€æµ‹å·¥å…·è°ƒç”¨
   */
  private detectToolCalls(content: string, allowedTools: string[]): ToolCall[] {
    const toolCalls: ToolCall[] = []
    
    // æ£€æµ‹æ ¼å¼ï¼šWrite("filename", "content")
    for (const toolName of allowedTools) {
      const regex = new RegExp(`${toolName}\\s*\\(\\s*"([^"]+)"(?:\\s*,\\s*"([^"]*)")?\\s*\\)`, 'gi')
      let match
      
      while ((match = regex.exec(content)) !== null) {
        const callId = `${toolName}_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`
        
        toolCalls.push({
          toolName,
          parameters: {
            file_path: match[1],
            content: match[2] || ''
          },
          callId
        })
      }
    }
    
    return toolCalls
  }

  /**
   * æ‰§è¡Œå·¥å…·è°ƒç”¨
   */
  private async executeToolCall(toolCall: ToolCall): Promise<AIToolExecutionResult> {
    try {
      const tool = getTool(toolCall.toolName)
      if (!tool) {
        // å…¼å®¹æ—§åŸŸå†™ä½œå·¥å…·ï¼ˆtodo_* ç­‰ï¼‰â€”â€”ç›´æ¥è°ƒç”¨å·¥å…·å®ç°
        const legacy = await this.executeLegacyTool(toolCall.toolName, toolCall.parameters)
        if (legacy) return { toolName: toolCall.toolName, callId: toolCall.callId, ...legacy }

        return {
          toolName: toolCall.toolName,
          callId: toolCall.callId,
          result: '',
          success: false,
          error: `å·¥å…· ${toolCall.toolName} ä¸å­˜åœ¨`
        }
      }

      // åˆ›å»ºå·¥å…·æ‰§è¡Œä¸Šä¸‹æ–‡
      const toolContext: ToolUseContext = {
        messageId: `ai-${toolCall.callId}`,
        agentId: 'ai-service',
        safeMode: false,
        abortController: new AbortController(),
        readFileTimestamps: {},
        options: {
          verbose: true,
          safeMode: false,
          messageLogName: 'ai-service'
        }
      }

      // æ‰§è¡Œå·¥å…· - ä½¿ç”¨ call æ–¹æ³•
      const generator = tool.call(toolCall.parameters, toolContext)
      const { value } = await generator.next()
      
      const result = value?.data || value

      return {
        toolName: toolCall.toolName,
        callId: toolCall.callId,
        result: typeof result === 'string' ? result : JSON.stringify(result),
        success: true
      }
    } catch (error) {
      return {
        toolName: toolCall.toolName,
        callId: toolCall.callId,
        result: '',
        success: false,
        error: error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'
      }
    }
  }

  /**
   * ç›´æ¥æ‰§è¡Œæ—§åŸŸå·¥å…·ï¼ˆæœªæ³¨å†Œåˆ°ç¼–æ’å™¨ï¼‰
   */
  private async executeLegacyTool(toolName: string, params: any): Promise<{ result: string; success: boolean; error?: string } | null> {
    try {
      // ç»Ÿä¸€ä¼šè¯ IDï¼Œç¡®ä¿ä¸ UI/CLI ä½¿ç”¨åŒä¸€ä¸ª Todo å­˜å‚¨
      const sessionId = process.env.WRITEFLOW_SESSION_ID
      const { TodoManager } = await import('../../tools/TodoManager.js')
      const sharedManager = new TodoManager(sessionId)

      if (toolName === 'todo_write') {
        const { TodoWriteTool } = await import('../../tools/writing/TodoWriteTool.js')
        const tool = new TodoWriteTool(sharedManager)
        const res = await tool.execute(params, { agentId: 'ai-service', abortController: new AbortController(), options: { verbose: false } })
        return { result: res.content || '', success: res.success, error: res.success ? undefined : (res as any).error }
      }
      if (toolName === 'todo_read') {
        const { TodoReadTool } = await import('../../tools/writing/TodoReadTool.js')
        const tool = new TodoReadTool(sharedManager)
        const res = await tool.execute(params, { agentId: 'ai-service', abortController: new AbortController(), options: { verbose: false } })
        return { result: res.content || '', success: res.success, error: res.success ? undefined : (res as any).error }
      }
      if (toolName === 'exit_plan_mode') {
        // ç®€åŒ–å¤„ç†ï¼šè¿”å›å›ºå®šæ¶ˆæ¯ï¼Œäº¤ç”±ä¸Šå±‚è§£æ
        return { result: 'å·²é€€å‡ºè®¡åˆ’æ¨¡å¼', success: true }
      }
      return null
    } catch (error) {
      return { result: '', success: false, error: (error as Error).message }
    }
  }

  /**
   * å°†å·¥å…·ç»“æœé›†æˆåˆ°å†…å®¹ä¸­
   */
  private integrateToolResults(content: string, results: AIToolExecutionResult[]): string {
    let updatedContent = content
    
    for (const result of results) {
      if (result.success) {
        // åœ¨å·¥å…·è°ƒç”¨ä½ç½®æ˜¾ç¤ºæ‰§è¡Œç»“æœ
        const toolCallPattern = new RegExp(`${result.toolName}\\s*\\([^)]+\\)`, 'gi')
        updatedContent = updatedContent.replace(toolCallPattern, (match) => {
          return `${match}\nâœ… [å·¥å…·æ‰§è¡Œå®Œæˆ] ${result.result}`
        })
      } else {
        // æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
        const toolCallPattern = new RegExp(`${result.toolName}\\s*\\([^)]+\\)`, 'gi')
        updatedContent = updatedContent.replace(toolCallPattern, (match) => {
          return `${match}\nâŒ [å·¥å…·æ‰§è¡Œå¤±è´¥] ${result.error}`
        })
      }
    }
    
    return updatedContent
  }
}

// å…¨å±€æœåŠ¡å®ä¾‹
let globalAIService: WriteFlowAIService | null = null

/**
 * è·å–å…¨å±€ AI æœåŠ¡å®ä¾‹
 */
export function getWriteFlowAIService(): WriteFlowAIService {
  if (!globalAIService) {
    globalAIService = new WriteFlowAIService()
  }
  return globalAIService
}

/**
 * å¿«é€Ÿ AI è¯·æ±‚å‡½æ•°
 */
export async function askAI(prompt: string, options?: Partial<AIRequest>): Promise<string> {
  const service = getWriteFlowAIService()
  const response = await service.processRequest({
    prompt,
    ...options
  })
  return response.content
}
